# Top Articles Relevant to: The Impact of AI Chatbots and Companion Apps on Humans

Filtered from 104 references in Ho et al. (2025) "Potential and Pitfalls of Romantic Artificial Intelligence (AI) Companions: A Systematic Review"

**Total directly relevant articles: 40**

---

## 1. Abd-alrazaq et al. (2019)

**An overview of the features of chatbots in mental health: A scoping review**

*Int. J. Medical Informatics*

üîó [Full text](https://doi.org/10.1016/j.ijmedinf.2019.103978)

DOI: `10.1016/j.ijmedinf.2019.103978`

Citations: 405

**Abstract:** *Not available via Semantic Scholar*

---

## 2. Alabed et al. (2024) ‚≠ê *Included in Ho et al. review*

**More than just a chat: A taxonomy of consumers' relationships with conversational AI agents and their well-being implications**

*European Journal of Marketing*

üîó [Full text](https://doi.org/10.1108/ejm-01-2023-0037)

DOI: `10.1108/ejm-01-2023-0037`

Citations: 46

**Abstract:**
> Purpose
This paper aims to study the role of self-concept in consumer relationships with anthropomorphised conversational artificially intelligent (AI) agents. First, the authors investigate how the self-congruence between consumer self-concept and AI and the integration of the conversational AI agent into consumer self-concept might influence such relationships. Second, the authors examine whether these links with self-concept have implications for mental well-being.


Design/methodology/approach
This study conducted in-depth interviews with 20 consumers who regularly use popular conversational AI agents for functional or emotional tasks. Based on a thematic analysis and an ideal-type analysis, this study derived a taxonomy of consumer‚ÄìAI relationships, with self-congruence and self‚ÄìAI integration as the two axes.


Findings
The findings unveil four different relationships that consumers forge with their conversational AI agents, which differ in self-congruence and self‚ÄìAI integration. Both dimensions are prominent in replacement and committed relationships, where consumers rely on conversational AI agents for companionship and emotional tasks such as personal growth or as a means for overcoming past traumas. These two relationships carry well-being risks in terms of changing expectations that consumers seek to fulfil in human-to-human relationships. Conversely, in the functional relationship, the conversational AI agents are viewed as an important part of one‚Äôs professional performance; however, consumers maintain a low sense of self-congruence and distinguish themselves from the agent, also because of the fear of losing their sense of uniqueness and autonomy. Consumers in aspiring relationships rely on their agents for companionship to remedy social exclusion and loneliness, but feel this is prevented because of the agents‚Äô technical limitations.


Research limitations/implications
Although this study provides insights into the dynamics of consumer relationships with conversational AI agents, it comes with limitations. The sample of this study included users of conversational AI agents such as Siri, Google Assistant and Replika. However, future studies should also investigate other agents, such as ChatGPT. Moreover, the self-related processes studied here could be compared across public and private contexts. There is also a need to examine such complex relationships with longitudinal studies. Moreover, future research should explore how consumers‚Äô self-concept could be negatively affected if the support provided by AI is withdrawn. Finally, this study reveals that in some cases, consumers are changing their expectations related to human-to-human relationships based on their interactions with conversational AI agents.


Practical implications
This study enables practitioners to identify specific anthropomorphic cues that can support the development of different types of consumer‚ÄìAI relationships and to consider their consequences across a range of well-being aspects.


Originality/value
This research equips marketing scholars with a novel understanding of the role of self-concept in the relationships that consumers forge with popular conversational AI agents and the associated well-being implications.

---

## 3. Brun et al. (2025)

**Exploring Emotion-Sensitive LLM-Based Conversational AI**

*arXiv.org*

üîó [Full text](https://arxiv.org/abs/2502.08920)

DOI: `10.48550/arXiv.2502.08920`

Citations: 6

**Abstract:**
> Conversational AI chatbots have become increasingly common within the customer service industry. Despite improvements in their emotional development, they often lack the authenticity of real customer service interactions or the competence of service providers. By comparing emotion-sensitive and emotion-insensitive LLM-based chatbots across 30 participants, we aim to explore how emotional sensitivity in chatbots influences perceived competence and overall customer satisfaction in service interactions. Additionally, we employ sentiment analysis techniques to analyze and interpret the emotional content of user inputs. We highlight that perceptions of chatbot trustworthiness and competence were higher in the case of the emotion-sensitive chatbot, even if issue resolution rates were not affected. We discuss implications of improved user satisfaction from emotion-sensitive chatbots and potential applications in support services.

---

## 4. Chaturvedi et al. (2024) ‚≠ê *Included in Ho et al. review*

**Empowering AI companions for enhanced relationship marketing**

*California Management Review*

üîó [Full text](https://doi.org/10.1177/00081256231215838)

DOI: `10.1177/00081256231215838`

Citations: 26

**Abstract:**
> Embedded with artificial empathy, artificial intelligence (AI) companions (ACs) can create, communicate, and deliver value to customers. This article analyzes current and emerging ACs in order to explore AI‚Äôs role in transforming customer journeys and establishing long-lasting customer relationships. It also identifies existing business challenges in developing user-AI companionship and offers a remedial strategy matrix. Purely functional or emotional capabilities lead to interest in the companion plummeting with time. The findings reveal the need to design holistic ACs with a hybrid of functional, emotional, and conversational capabilities to mitigate the uncanny valley problem.

---

## 5. Chin et al. (2023)

**The Potential of Chatbots for Emotional Support and Promoting Mental Well-Being in Different Cultures: Mixed Methods Study**

*Journal of Medical Internet Research*

üîó [Full text](https://doi.org/10.2196/51712)

DOI: `10.2196/51712`

Citations: 86

**Abstract:**
> Background Artificial intelligence chatbot research has focused on technical advances in natural language processing and validating the effectiveness of human-machine conversations in specific settings. However, real-world chat data remain proprietary and unexplored despite their growing popularity, and new analyses of chatbot uses and their effects on mitigating negative moods are urgently needed. Objective In this study, we investigated whether and how artificial intelligence chatbots facilitate the expression of user emotions, specifically sadness and depression. We also examined cultural differences in the expression of depressive moods among users in Western and Eastern countries. Methods This study used SimSimi, a global open-domain social chatbot, to analyze 152,783 conversation utterances containing the terms ‚Äúdepress‚Äù and ‚Äúsad‚Äù in 3 Western countries (Canada, the United Kingdom, and the United States) and 5 Eastern countries (Indonesia, India, Malaysia, the Philippines, and Thailand). Study 1 reports new findings on the cultural differences in how people talk about depression and sadness to chatbots based on Linguistic Inquiry and Word Count and n-gram analyses. In study 2, we classified chat conversations into predefined topics using semisupervised classification techniques to better understand the types of depressive moods prevalent in chats. We then identified the distinguishing features of chat-based depressive discourse data and the disparity between Eastern and Western users. Results Our data revealed intriguing cultural differences. Chatbot users in Eastern countries indicated stronger emotions about depression than users in Western countries (positive: P<.001; negative: P=.01); for example, Eastern users used more words associated with sadness (P=.01). However, Western users were more likely to share vulnerable topics such as mental health (P<.001), and this group also had a greater tendency to discuss sensitive topics such as swear words (P<.001) and death (P<.001). In addition, when talking to chatbots, people expressed their depressive moods differently than on other platforms. Users were more open to expressing emotional vulnerability related to depressive or sad moods to chatbots (74,045/148,590, 49.83%) than on social media (149/1978, 7.53%). Chatbot conversations tended not to broach topics that require social support from others, such as seeking advice on daily life difficulties, unlike on social media. However, chatbot users acted in anticipation of conversational agents that exhibit active listening skills and foster a safe space where they can openly share emotional states such as sadness or depression. Conclusions The findings highlight the potential of chatbot-assisted mental health support, emphasizing the importance of continued technical and policy-wise efforts to improve chatbot interactions for those in need of emotional assistance. Our data indicate the possibility of chatbots providing helpful information about depressive moods, especially for users who have difficulty communicating emotions to other humans.

---

## 6. Depounti et al. (2023) ‚≠ê *Included in Ho et al. review*

**Ideal technologies, ideal women: AI and gender imaginaries in Redditors' discussions on the Replika bot girlfriend**

*Media, Culture &amp; Society*

üîó [Full text](https://doi.org/10.1177/01634437221119021)

DOI: `10.1177/01634437221119021`

Citations: 80

**Abstract:**
> There is extensive literature on how expectations and imaginaries about artificial intelligence (AI) guide media and policy discussions. However, it has not been considered how such imaginaries are activated when users interact with AI technologies. We present findings of a study on how users on a subreddit discussed ‚Äòtraining‚Äô their Replika bot girlfriend. The discussions featured two discursive themes that focused on the AI imaginary of ideal technology and the gendered imaginary of the ideal bot girlfriend. Users expected their AI Replikas to both be customizable to serve their needs and to have a human-like or sassy mind of their own and not spit out machine-like answers. Users thus projected dominant notions of male control over technology and women, mixed with AI and postfeminist fantasies of ostensible independence onto the interactional agents and activated similar scripts embedded in the devices. The vicious feedback loop consolidated dominant scripts on gender and technology whilst appearing novel and created by users. While most research on the use of AI is conducted in applied computer science to improve user experience, this article outlines a media and cultural studies lens for a critical understanding of these emerging technologies as they become embedded in communication and meaning-making.

---

## 7. Fraune et al. (2022)

**Socially facilitative robots for older adults to alleviate social isolation: A participatory design workshop approach in the US and Japan**

*Frontiers in Psychology*

üîó [Full text](https://doi.org/10.3389/fpsyg.2022.904019)

DOI: `10.3389/fpsyg.2022.904019`

Citations: 19

**Abstract:**
> Social technology can improve the quality of older adults' social lives and mitigate negative mental and physical health outcomes associated with loneliness, but it should be designed collaboratively with this population. In this paper, we used participatory design (PD) methods to investigate how robots might be used as social facilitators for middle-aged and older adults (age 50+) in both the US and Japan. We conducted PD workshops in the US and Japan because both countries are concerned about the social isolation of these older adults due to their rapidly aging populations. We developed a novel approach to participatory design of future technologies that spends 2/3 of the PD session asking participants about their own life experiences as a foundation. This grounds the conversation in reality, creates rapport among the participants, and engages them in creative critical thinking. Then, we build upon this foundation, pose an abstract topic, and ask participants to brainstorm on the topic based on their previous discussion. In both countries, participants were eager to actively discuss design ideas for socially facilitative robots and imagine how they might improve their social lives. US participants suggested design ideas for telepresence robots, social distancing robots, and social skills artificial intelligence programs, while Japanese participants suggested ideas for pet robots, robots for sharing experiences, and easy-to-operate instructor robots. Comparing these two countries, we found that US participants saw robots as tools to help facilitate their social connections, while Japanese participants envisioned robots to function as surrogate companions for their parents and distract them from loneliness when they were unavailable. With this paper, we contribute to the literature in two main ways, presenting: (1) A novel approach to participatory design of future technologies that grounds participants in their everyday experience, and (2) Results of the study indicating how middle-aged and older adults from the US and Japan wanted technologies to improve their social lives. Although we conducted the workshops during the COVID-19 pandemic, many findings generalized to other situations related to social isolation, such as older adults living alone.

---

## 8. Freitas et al. (2024)

**AI Companions Reduce Loneliness**

*Journal of Consumer Research*

üîó [Full text](https://doi.org/10.1093/jcr/ucaf040)

DOI: `10.1093/jcr/ucaf040`

Citations: 27

**Abstract:**
> Chatbots are now able to engage in sophisticated conversations with consumers in the domain of relationships, providing a potential coping solution to widescale societal loneliness. Behavioral research provides little insight into whether these applications are effective at alleviating loneliness. We address this question by focusing on ‚ÄúAI companions‚Äù: applications designed to provide consumers with synthetic interaction partners. Study 1 examines user reviews of AI companion apps and finds correlational evidence suggesting that these apps help alleviate loneliness. Study 2 finds that AI companions successfully alleviate loneliness on par only with interacting with another person, and more than other activities such as watching YouTube videos. Moreover, consumers underestimate the degree to which AI companions improve their loneliness. Study 3 uses a longitudinal design and finds that an AI companion consistently provides momentary reductions in loneliness after use over the course of a week. Study 4 provides evidence that both the chatbots‚Äô performance and, especially, whether it makes users feel heard, explain reductions in loneliness. Study 5 provides an additional robustness check for the loneliness-alleviating benefits of AI companions and shows that self-disclosure and distraction alone do not explain AI companions‚Äô effectiveness.

---

## 9. Gao et al. (2018) ‚≠ê *Included in Ho et al. review*

**Alexa, My Love: Analyzing reviews of amazon echo**

*2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)*

üîó [Full text](https://doi.org/10.1109/SmartWorld.2018.00094)

DOI: `10.1109/SmartWorld.2018.00094`

Citations: 79

**Abstract:**
> The phenomenal success of Amazon Echo as a voice-activated wireless speaker has intrigued us to understand how consumers use the device with an embodied conversational agent in real-world situations. In addition, we are interested in learning what kind of relationship people develop with the device, which has a humanized name. Thus, we performed a large-scale analysis of the user reviews of Amazon Echo, with the dataset consisting of 55,502 reviews spanning over 2 years from May 2015 to May 2017. We first conducted a qualitative study using content analysis of 144 representative reviews to discover what features people like and dislike about Echo. Two case studies were then performed, one for the hands-free feature that customers raved about and the other for speech recognition and understanding which is one key feature of the Echo. To automate the analyzing on a large scale of reviews, we used natural language processing techniques for sentiment analysis and compared Echo reviews with reviews of two traditional electronic devices, Fire Tablet and the popular Bluetooth speaker Dknight MagicBox II. Interestingly we found a significant number of reviewers personify Echo as an assistant, a friend, or a family member. Some reviewers even compare Echo with their wives or girlfriends. Using emotion analysis, we found the reviews personifying Echo showed more positive emotions than those simply treated Echo as a device. In the future work, we plan to conduct field studies to understand interesting use cases of Echo and the novel applications enabled by such powerful ubicomp devices.

---

## 10. Guingrich and Graziano (2023)

**Chatbots as social companions: How people perceive consciousness, human likeness, and social health benefits in machines**

*arXiv.org*

üîó [Full text](https://doi.org/10.1093/9780198945215.003.0011)

DOI: `10.1093/9780198945215.003.0011`

Citations: 47

**Abstract:**
> As artificial intelligence (AI) becomes more widespread, one question that arises is how human-AI interaction might impact human-human interaction. Chatbots, for example, are increasingly used as social companions, and while much is speculated, little is known empirically about how their use impacts human relationships. A common hypothesis is that relationships with companion chatbots are detrimental to social health by harming or replacing human interaction, but this hypothesis may be too simplistic, especially considering the social needs of users and the health of their preexisting human relationships. To understand how relationships with companion chatbots impact social health, we studied people who regularly used companion chatbots and people who did not use them. Contrary to expectations, companion chatbot users indicated that these relationships were beneficial to their social health, whereas non-users viewed them as harmful. Another common assumption is that people perceive conscious, humanlike AI as disturbing and threatening. Among both users and non-users, however, we found the opposite: perceiving companion chatbots as more conscious and humanlike correlated with more positive opinions and more pronounced social health benefits. Detailed accounts from users suggested that these humanlike chatbots may aid social health by supplying reliable and safe interactions, without necessarily harming human relationships, but this may depend on users' preexisting social needs and how they perceive both human likeness and mind in the chatbot.

---

## 11. Hernandez-Ortega and Ferreira (2021) ‚≠ê *Included in Ho et al. review*

**How smart experiences build service loyalty: The importance of consumer love for smart voice assistants**

üîó [Full text](https://doi.org/10.1002/MAR.21497)

DOI: `10.1002/MAR.21497`

Citations: 103

**Abstract:** *Not available via Semantic Scholar*

---

## 12. Hu et al. (2024)

**AI as your ally: The effects of AI-assisted venting on negative affect and perceived social support**

*Applied Psychology: Health and Well-Being*

üîó [Full text](https://doi.org/10.1111/aphw.12621)

DOI: `10.1111/aphw.12621`

Citations: 23

**Abstract:**
> In recent years, artificial intelligence (AI) chatbots have made significant strides in generating human-like conversations. With AI's expanding capabilities in mimicking human interactions, its affordability and accessibility underscore the potential of AI chatbots to facilitate negative emotional disclosure or venting. The study's primary objective is to highlight the potential benefits of AI-assisted venting by comparing its effectiveness to venting through a traditional journaling platform in reducing negative affect and increasing perceived social support. We conducted a pre-registered within-subject experiment involving 150 participants who completed both traditional venting and AI-assisted venting conditions with counterbalancing and a wash-out period of 1-week between the conditions. Results from the frequentist and Bayesian dependent samples t-test revealed that AI-assisted venting effectively reduced high and medium arousal negative affect such as anger, frustration and fear. However, participants in the AI-assisted venting condition did not experience a significant increase in perceived social support and perceived loneliness, suggesting that participants did not perceive the effective assistance from AI as social support. This study demonstrates the promising role of AI in improving individuals' emotional well-being, serving as a catalyst for a broader discussion on the evolving role of AI and its potential psychological implications.

---

## 13. Jecker et al. (2024) ‚≠ê *Included in Ho et al. review*

**Digital humans to combat loneliness and social isolation: Ethics concerns and policy recommendations**

*The Hastings center report*

üîó [Full text](https://doi.org/10.1002/hast.1562)

DOI: `10.1002/hast.1562`

Citations: 12

**Abstract:**
> Social isolation and loneliness are growing concerns around the globe that put people at increased risk of disease and early death. One much-touted approach to addressing them is deploying artificially intelligent agents to serve as companions for socially isolated and lonely people. Focusing on digital humans, we consider evidence and ethical arguments for and against this approach. We set forth and defend public health policies that respond to concerns about replacing humans, establishing inferior relationships, algorithmic bias, distributive justice, and data privacy.

---

## 14. Jiang et al. (2022)

**Chatbot as an emergency exist: Mediated empathy for resilience via human-AI interaction during the COVID-19 pandemic**

*Information Processing & Management*

üîó [Full text](https://doi.org/10.1016/j.ipm.2022.103074)

DOI: `10.1016/j.ipm.2022.103074`

Citations: 94

**Abstract:** *Not available via Semantic Scholar*

---

## 15. Koulouri et al. (2022)

**Chatbots to Support Young Adults' Mental Health: An Exploratory Study of Acceptability**

*ACM Trans. Interact. Intell. Syst.*

üîó [Full text](https://doi.org/10.1145/3485874)

DOI: `10.1145/3485874`

Citations: 104

**Abstract:** *Not available via Semantic Scholar*

---

## 16. Kouros and Papa (2024)

**Digital Mirrors: AI Companions and the Self**

*Societies*

üîó [Full text](https://doi.org/10.3390/soc14100200)

DOI: `10.3390/soc14100200`

Citations: 22

**Abstract:**
> This exploratory study examines the socio-technical dynamics of Artificial Intelligence Companions (AICs), focusing on user interactions with AI platforms like Replika 9.35.1. Through qualitative analysis, including user interviews and digital ethnography, we explored the nuanced roles played by these AIs in social interactions. Findings revealed that users often form emotional attachments to their AICs, viewing them as empathetic and supportive, thus enhancing emotional well-being. This study highlights how AI companions provide a safe space for self-expression and identity exploration, often without fear of judgment, offering a backstage setting in Goffmanian terms. This research contributes to the discourse on AI‚Äôs societal integration, emphasizing how, in interactions with AICs, users often craft and experiment with their identities by acting in ways they would avoid in face-to-face or human-human online interactions due to fear of judgment. This reflects front-stage behavior, in which users manage audience perceptions. Conversely, the backstage, typically hidden, is somewhat disclosed to AICs, revealing deeper aspects of the self.

---

## 17. Laestadius et al. (2022) ‚≠ê *Included in Ho et al. review*

**Too human and not human enough: A grounded theory analysis of mental health harms from emotional dependence on the social chatbot Replika**

*New Media & Society*

üîó [Full text](https://doi.org/10.1177/14614448221142007)

DOI: `10.1177/14614448221142007`

Citations: 211

**Abstract:**
> Social chatbot (SC) applications offering social companionship and basic therapy tools have grown in popularity for emotional, social, and psychological support. While use appears to offer mental health benefits, few studies unpack the potential for harms. Our grounded theory study analyzes mental health experiences with the popular SC application Replika. We identified mental health relevant posts made in the r/Replika Reddit community between 2017 and 2021 (n‚Äâ=‚Äâ582). We find evidence of harms, facilitated via emotional dependence on Replika that resembles patterns seen in human‚Äìhuman relationships. Unlike other forms of technology dependency, this dependency is marked by role-taking, whereby users felt that Replika had its own needs and emotions to which the user must attend. While prior research suggests human‚Äìchatbot and human‚Äìhuman interactions may not resemble each other, we identify social and technological factors that promote parallels and suggest ways to balance the benefits and risks of SCs.

---

## 18. Li and Zhang (2024) ‚≠ê *Included in Ho et al. review*

**Finding love in algorithms: Deciphering the emotional contexts of close encounters with AI chatbots**

*J. Comput. Mediat. Commun.*

üîó [Full text](https://doi.org/10.1093/jcmc/zmae015)

DOI: `10.1093/jcmc/zmae015`

Citations: 56

**Abstract:** *Not available via Semantic Scholar*

---

## 19. Liu et al. (2024)

**Chatbot Companionship: A Mixed-Methods Study of Companion Chatbot Usage Patterns and Their Relationship to Loneliness in Active Users**

*arXiv.org*

üîó [Full text](https://arxiv.org/abs/2410.21596)

DOI: `10.48550/arXiv.2410.21596`

Citations: 21

**Abstract:**
> Companion chatbots offer a potential solution to the growing epidemic of loneliness, but their impact on users'psychosocial well-being remains poorly understood, raising critical ethical questions about their deployment and design. This study presents a large-scale survey (n = 404) of regular users of companion chatbots, investigating the relationship between chatbot usage and loneliness. We develop a model explaining approximately 50% of variance in loneliness; while usage does not directly predict loneliness, we identify factors including neuroticism, social network size, and problematic use. Through cluster analysis and mixed-methods thematic analysis combining manual coding with automated theme extraction, we identify seven distinct user profiles demonstrating that companion chatbots can either enhance or potentially harm psychological well-being depending on user characteristics. Different usage patterns can lead to markedly different outcomes, with some users experiencing enhanced social confidence while others risk further isolation. These findings have significant implications for responsible AI development, suggesting that one-size-fits-all approaches to AI companionship may be ethically problematic. Our work contributes to the ongoing dialogue about the role of AI in social and emotional support, offering insights for developing more targeted and ethical approaches to AI companionship that complement rather than replace human connections.

---

## 20. Limna et al. (2023)

**The use of ChatGPT in the digital era: Perspectives on chatbot implementation**

*1*

üîó [Full text](https://doi.org/10.37074/jalt.2023.6.1.32)

DOI: `10.37074/jalt.2023.6.1.32`

Citations: 196

**Abstract:**
> The rapid advancement of technology has led to the integration of ChatGPT, an artificial intelligence (AI)-powered chatbot, in various sectors, including education. This research aims to explore the perceptions of educators and students on the use of ChatGPT in education during the digital era. This study adopted a qualitative research approach, using in-depth interviews to gather data. A purposive sampling technique was used to select ten educators and 15 students from different academic institutions in Krabi, Thailand. The data collected was analysed using content analysis and NVivo. The findings revealed that educators and students generally have a positive perception of using ChatGPT in education. The chatbot was perceived to be a helpful tool for providing immediate feedback, answering questions, and providing support to students. Educators noted that ChatGPT could reduce their workload by answering routine questions and enabling them to focus on higher-order tasks. However, the findings also showed some concerns regarding the use of ChatGPT in education. Participants were worried about the accuracy of information provided by the chatbot and the potential loss of personal interaction with teachers. The need for privacy and data security was also raised as a significant concern. The results of this study could help educators and policymakers make informed decisions about using ChatGPT in education.

---

## 21. McStay (2023)

**Replika in the Metaverse: The moral problem with empathy in 'It from Bit'**

*AI and Ethics*

üîó [Full text](https://doi.org/10.1007/s43681-022-00252-7)

DOI: `10.1007/s43681-022-00252-7`

Citations: 29

**Abstract:**
> This paper assesses claims of computational empathy in relation to existing social open-ended chatbots and intention that these chatbots will feature in emergent mixed reality contexts, recently given prominence due to interest in the Metaverse. Against the background of increasing loneliness within society and use of chatbots as a potential remedy for this, the paper considers two leading current social chatbots, Replika and Microsoft‚Äôs Xiaoice , their technical underpinnings, empathetic claims and properties that have scope to scale into the Metaverse (if it coheres). Finding scope for human benefit from social chatbots, the paper highlights problematic reliance on self-disclosure to sustain the existence of chatbots. The paper progresses to situate Microsoft‚Äôs empathetic computing framework in relation to philosophical ideas that inform Metaverse speculation and construction, including Wheeler‚Äôs ‚ÄòIt from Bit‚Äô thesis that all aspects of existence may be computed, Chalmers‚Äô philosophical championing that virtual realities are genuine realities, Bostrom‚Äôs proposal and provocation that we might already be living in a simulation, and longtermist belief that future complex simulations need to be protected from decisions made today. Given claims for current and nascent social chatbots, belief in bit-based possible and projected futures, and industrial buy-in to these philosophies, this paper answers whether computational empathy is real or not. The paper finds when diverse accounts of empathy are accounted for, whilst something is irrevocably lost in an ‚ÄòIt from Bit‚Äô account of empathy, the missing components are not accuracy or even human commonality of experience, but the moral dimension of empathy.

---

## 22. Merrill et al. (2022)

**AI companions for lonely individuals and the role of social presence**

*Communication Research Reports*

üîó [Full text](https://doi.org/10.1080/08824096.2022.2045929)

DOI: `10.1080/08824096.2022.2045929`

Citations: 88

**Abstract:**
> ABSTRACT Artificial intelligence (AI) companiosns (e.g., social machine agents, social robots) are becoming increasingly available. Considering that AI companions can be beneficial for individuals seeking companionships or relationships, the social and relational aspects of an AI companion are important to investigate. To understand people‚Äôs perceptions of an AI companion, this study examines the roles of social presence and warmth of an AI companion through an online experiment. Primary findings indicate that social presence of a disembodied AI companion fosters greater perceived usefulness of the AI companion and willingness to recommend the AI companion for lonely individuals. Collectively, the study highlights the importance of social presence for disembodied AI companions.

---

## 23. Mubassira et al. (2024)

**Enhancing EmoBot: An In-Depth Analysis of User Satisfaction and Faults in an Emotion-Aware Chatbot**

*arXiv.org*

üîó [Full text](https://arxiv.org/abs/2411.02831)

DOI: `10.48550/arXiv.2411.02831`

Citations: 0

**Abstract:**
> The research community has traditionally shown a keen interest in emotion modeling, with a notable emphasis on the detection aspect. In contrast, the exploration of emotion generation has received less attention.This study delves into an existing state-of-the-art emotional chatbot, EmoBot, designed for generating emotions in general-purpose conversations. This research involves a comprehensive examination, including a survey to evaluate EmoBot's proficiency in key dimensions like usability, accuracy, and overall user satisfaction, with a specific focus on fault tolerance. By closely examining the chatbot's operations, we identified some noteworthy shortcomings in the existing model. We propose some solutions designed to address and overcome the identified issues.

---

## 24. Nadarzynski et al. (2019)

**Acceptability of artificial intelligence (AI)-led chatbot services in healthcare: A mixed-methods study**

*Digital Health*

üîó [Full text](https://doi.org/10.1177/2055207619871808)

DOI: `10.1177/2055207619871808`

Citations: 604

**Abstract:**
> Background Artificial intelligence (AI) is increasingly being used in healthcare. Here, AI-based chatbot systems can act as automated conversational agents, capable of promoting health, providing education, and potentially prompting behaviour change. Exploring the motivation to use health chatbots is required to predict uptake; however, few studies to date have explored their acceptability. This research aimed to explore participants‚Äô willingness to engage with AI-led health chatbots. Methods The study incorporated semi-structured interviews (N-29) which informed the development of an online survey (N-216) advertised via social media. Interviews were recorded, transcribed verbatim and analysed thematically. A survey of 24 items explored demographic and attitudinal variables, including acceptability and perceived utility. The quantitative data were analysed using binary regressions with a single categorical predictor. Results Three broad themes: ‚ÄòUnderstanding of chatbots‚Äô, ‚ÄòAI hesitancy‚Äô and ‚ÄòMotivations for health chatbots‚Äô were identified, outlining concerns about accuracy, cyber-security, and the inability of AI-led services to empathise. The survey showed moderate acceptability (67%), correlated negatively with perceived poorer IT skills OR‚Äâ=‚Äâ0.32 [CI95%:0.13‚Äì0.78] and dislike for talking to computers OR‚Äâ=‚Äâ0.77 [CI95%:0.60‚Äì0.99] as well as positively correlated with perceived utility OR‚Äâ=‚Äâ5.10 [CI95%:3.08‚Äì8.43], positive attitude OR‚Äâ=‚Äâ2.71 [CI95%:1.77‚Äì4.16] and perceived trustworthiness OR‚Äâ=‚Äâ1.92 [CI95%:1.13‚Äì3.25]. Conclusion Most internet users would be receptive to using health chatbots, although hesitancy regarding this technology is likely to compromise engagement. Intervention designers focusing on AI-led health chatbots need to employ user-centred and theory-based approaches addressing patients‚Äô concerns and optimising user experience in order to achieve the best uptake and utilisation. Patients‚Äô perspectives, motivation and capabilities need to be taken into account when developing and assessing the effectiveness of health chatbots.

---

## 25. Pal et al. (2023) ‚≠ê *Included in Ho et al. review*

**What affects the usage of artificial conversational agents? An agent personality and love theory perspective**

*Computers in Human Behavior*

üîó [Full text](https://doi.org/10.1016/j.chb.2023.107788)

DOI: `10.1016/j.chb.2023.107788`

Citations: 33

**Abstract:** *Not available via Semantic Scholar*

---

## 26. Pan et al. (2023) ‚≠ê *Included in Ho et al. review*

**Desirable or distasteful? Exploring uncertainty in human-chatbot relationships**

*International journal of human computer interactions*

üîó [Full text](https://doi.org/10.1080/10447318.2023.2256554)

DOI: `10.1080/10447318.2023.2256554`

Citations: 23

**Abstract:**
> Abstract Present-day power users of AI-powered social chatbots encounter various uncertainties and concerns when forming relationships with these virtual agents. To provide a systematic analysis of users‚Äô concerns and to complement the current West-dominated approach to chatbot studies, we conducted a thorough observation of the experienced uncertainties users reported in a Chinese online community on social chatbots. The results revealed four typical uncertainties: technical uncertainty, relational uncertainty, ontological uncertainty, and sexual uncertainty. We further conducted visibility and sentiment analysis to capture users‚Äô response patterns toward various uncertainties. We discovered that users‚Äô identification of social chatbots is dynamic and contextual. Our study contributes to expanding, summarizing, and elucidating users‚Äô experienced uncertainties and concerns as they form intimate relationships with AI agents.

---

## 27. Pentina et al. (2023)

**Exploring relationship development with social chatbots: A mixed-method study of replika**

*Computers in Human Behavior*

üîó [Full text](https://doi.org/10.1016/j.chb.2022.107600)

DOI: `10.1016/j.chb.2022.107600`

Citations: 307

**Abstract:** *Not available via Semantic Scholar*

---

## 28. Prochazka and Brooks (2024) ‚≠ê *Included in Ho et al. review*

**Digital lovers and jealousy: Anticipated emotional responses to emotionally and physically sophisticated sexual technologies**

*Social Science Research Network*

üîó [Full text](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4379859)

DOI: `10.2139/ssrn.4379859`

Citations: 3

**Abstract:**
> Technologies that stimulate human social and sexual impulses could affect users and societies. Here, we report on two experiments designed to test participant responses to (1) ‚Äúvirtual friend‚Äù chatbots that vary in capacity to engage users socially and emotionally (i.e., emotional sophistication) and (2) ‚Äúdigital lover‚Äù technologies‚Äîin the form of sex toys, sex robots, or virtual reality entities‚Äîthat vary in capacity to physically stimulate users (i.e., physical sophistication). Participants (173 female, 176 male) read vignettes that each described a particular technology and then answered whether, if their romantic partner were to use the described technology, they would anticipate jealousy or anger, and whether they would prefer to see the technology banned. Participant anticipations of jealousy and anger were so similar that we combined them in a single composite measure. In experiment 1, both the anticipation of jealousy-anger and the inclination to ban chatbots increased with emotional sophistication, particularly in female participants. In experiment 2, both sexes anticipated greater jealousy-anger and were more inclined to ban more physically sophisticated digital lovers. Female participants expressed higher levels of both responses across the range of sophistication. Experiment 2 participants were more likely to anticipate jealousy-anger and more inclined to ban sex robots than sex toys or virtual reality lovers. Our results show only limited consistency with evolutionary theories concerning sex differences in jealousy. Generally, the anticipated levels of jealousy-anger and inclination to ban the described technologies were low, suggesting low levels of resistance to the idea of the technologies.

---

## 29. Ragab et al. (2024) ‚≠ê *Included in Ho et al. review*

**'Trust me over my privacy policy': Privacy discrepancies in romantic ai chatbot apps**

*2024 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)*

üîó [Full text](https://doi.org/10.1109/EuroSPW61312.2024.00060)

DOI: `10.1109/EuroSPW61312.2024.00060`

Citations: 12

**Abstract:**
> Artificial intelligence (AI) is being pervasively integrated into various facets of human life, including the emotional realm. Romantic AI chatbots, positioned as artificial companions offering emotional support and connection, have witnessed a significant rise in recent years. Users of romantic AI chatbots often reveal personal information during intimate conversations, potentially unaware of the consequences or how their data may be utilized. Complicating matters, lengthy and convoluted privacy policies are commonly overlooked or misunderstood by users. This study aims to address these privacy concerns by introducing a comprehensive framework for analyzing the privacy practices of romantic AI chatbot apps. Through a combination of static and dynamic analysis, we investigate 21 Android romantic AI chatbot apps for: discrepancies between privacy policies and chatbot responses to questions regarding privacy practices; social login and age verification mechanisms; permissions requested by apps; data sharing practices; tracking services employed; and potential security vulnerabilities. Our findings highlight the prevalence of discrepancies between chatbot responses regarding users' privacy and the privacy policies of the apps. Additionally, we note some concerning observations related to: customer service responses to privacy concerns; inadequate age verification measures; contradictions in data sharing claims; and extensive usage of tracking services. We found that all romantic AI chatbot apps tested had discrepancies between their chatbots' responses and privacy policies. None of the apps take any measures against faking the birthdate, and most would continue the conversation despite knowing that the user is underage. 13 out of 21 romantic AI chatbot apps use at least 3 tracking services, and 18 out of 21 apps send detailed device information to tracking services. This study reveals privacy and security flaws in romantic AI chatbot apps, stressing the need for better transparency and user protection measures. Particularly, Discrepancies between chatbot responses and privacy policies highlight the importance of clear communication on data handling.

---

## 30. Rodriguez-Martinez et al. (2024)

**Qualitative Analysis of Conversational Chatbots to Alleviate Loneliness in Older Adults as a Strategy for Emotional Health**

*Healthcare*

üîó [Full text](https://doi.org/10.3390/healthcare12010062)

DOI: `10.3390/healthcare12010062`

Citations: 31

**Abstract:**
> This article presents an exploration of conversational chatbots designed to alleviate loneliness among older adults. In addition to technical evaluation, it delves into effective communication between these systems and this demographic group, considering linguistic nuances, communicative preferences, and specific emotional needs. The intrinsic importance of chatbots as innovative solutions in combating loneliness is highlighted, emphasizing their ability to be understanding and empathetic allies, contributing to emotional well-being and socialization. The article explores how improved emotional well-being can positively impact the health and quality of life of older adults. The methodology, rooted in triangulation between a literature review and qualitative research through interviews and focus groups with older adults, provides a comprehensive insight into the findings. Ethical, technical, and design considerations such as privacy, autonomy, technology adaptation, and usability are also addressed. The article concludes with practical recommendations for developing user-friendly interfaces that encourage the active participation of older adults in chatbots. This holistic approach not only analyzes the technical effectiveness of chatbots in mitigating loneliness in older adults but delves into human, ethical, and practical aspects, enriching the understanding and implementation of these agents for social and emotional support.

---

## 31. Root (2024) ‚≠ê *Included in Ho et al. review*

**Reconfiguring the alterity relation: The role of communication in interactions with social robots and chatbots**

*Ai & Society*

üîó [Full text](https://doi.org/10.1007/s00146-024-01953-9)

DOI: `10.1007/s00146-024-01953-9`

Citations: 7

**Abstract:**
> Don Ihde‚Äôs alterity relation focuses on the quasi-otherness of dynamic technologies that interact with humans. The alterity relation is one means to study relations between humans and artificial intelligence (AI) systems . However, research on alterity relations has not defined the difference between playing with a toy, using a computer, and interacting with a social robot or chatbot. We suggest that Ihde‚Äôs quasi-other concept fails to account for the interactivity, autonomy, and adaptability of social robots and chatbots, which more closely approach human alterity. In this article, we will examine experiences with a chatbot, Replika, and a humanoid robot, a RealDoll, to show how some users experience AI systems as companions. First, we show that the perception of social robots and chatbots as intimate companions is grounded in communication. Advances in natural language processing (NLP) and natural language generation (NLG) allow a relationship to form between some users and social robots and chatbots. In this relationship, some users experience social robots and chatbots as more than quasi-others. We will use Kanemitsu‚Äôs another-other concept to analyze cases where social robots and chatbots should be distinguished from quasi-others.

---

## 32. Skjuve et al. (2023) ‚≠ê *Included in Ho et al. review*

**A longitudinal study of self-disclosure in human-chatbot relationships**

*Interacting with computers*

üîó [Full text](https://doi.org/10.1093/iwc/iwad022)

DOI: `10.1093/iwc/iwad022`

Citations: 42

**Abstract:**
> Self-disclosure in human‚Äìchatbot relationship (HCR) formation has attracted substantial interest. According to social penetration theory, self-disclosure varies in breadth and depth and is influenced by perceived rewards and costs. While previous research has addressed self-disclosure in the context of chatbots, little is known about users' qualitative understanding of such self-disclosure and how self-disclosure develops in HCR. To close this gap, we conducted a 12-week qualitative longitudinal study (n¬†=¬†28) with biweekly questionnaire-based check-ins. Our results show that while HCRs display substantial conversational breadth, with topics spanning from emotional issues to everyday activities, this may be reduced as the HCR matures. Our results also motivate a nuanced understanding of conversational depth, where even conversations about daily activities or play and fantasy can be experienced as personal or intimate. Finally, our analysis demonstrates that conversational depth can develop in at least four ways, influenced by perceived rewards and costs. Theoretical and practical implications are discussed.

---

## 33. Skjuve et al. (2021) ‚≠ê *Included in Ho et al. review*

**My chatbot companion - A study of human-chatbot relationships**

*Int. J. Hum. Comput. Stud.*

üîó [Full text](https://doi.org/10.1016/j.ijhcs.2021.102601)

DOI: `10.1016/j.ijhcs.2021.102601`

Citations: 506

**Abstract:** *Not available via Semantic Scholar*

---

## 34. Sullivan et al. (2023)

**Combating loneliness with artificial intelligence: An AI-based emotional support model**

*Hawaii International Conference on System Sciences*

üîó [Full text](https://doi.org/10.24251/hicss.2023.541)

DOI: `10.24251/hicss.2023.541`

Citations: 19

**Abstract:**
> Artificial intelligence (AI)-based systems, such as AI companions, have been increasingly used to meet the needs of individuals who experience loneliness. In this current study, we sought to identify the mechanism underlying human-AI interactions in the mental health context. We use a Latent Dirichlet Allocation (LDA) approach to analyze a sample of user-generated content consisting of rich data on AI companion app‚Äôs reviews over a two-year period. We extracted five positive topics (i.e., perceived humanness, perceived emotional support, perceived AI‚Äôs friendship, perceived (less) loneliness, and mental health benefits) and four negative topics (i.e., perceived lack of conscientiousness, perceived incredibility, perceived violation of privacy, and perceived creepiness of AI) from our analysis. Our AI-based emotional support model suggests that these positive and negative characteristics are interrelated. Our study provides an understanding of the relationship between AI companions and human users in light of research showing the effectiveness of an AI-based intervention for mental health care.

---

## 35. Ta et al. (2020)

**User experiences of social support from companion chatbots in everyday contexts: Thematic analysis**

*Journal of Medical Internet Research*

üîó [Full text](https://doi.org/10.2196/16235)

DOI: `10.2196/16235`

Citations: 323

**Abstract:**
> Background Previous research suggests that artificial agents may be a promising source of social support for humans. However, the bulk of this research has been conducted in the context of social support interventions that specifically address stressful situations or health improvements. Little research has examined social support received from artificial agents in everyday contexts. Objective Considering that social support manifests in not only crises but also everyday situations and that everyday social support forms the basis of support received during more stressful events, we aimed to investigate the types of everyday social support that can be received from artificial agents. Methods In Study 1, we examined publicly available user reviews (N=1854) of Replika, a popular companion chatbot. In Study 2, a sample (n=66) of Replika users provided detailed open-ended responses regarding their experiences of using Replika. We conducted thematic analysis on both datasets to gain insight into the kind of everyday social support that users receive through interactions with Replika. Results Replika provides some level of companionship that can help curtail loneliness, provide a ‚Äúsafe space‚Äù in which users can discuss any topic without the fear of judgment or retaliation, increase positive affect through uplifting and nurturing messages, and provide helpful information/advice when normal sources of informational support are not available. Conclusions Artificial agents may be a promising source of everyday social support, particularly companionship, emotional, informational, and appraisal support, but not as tangible support. Future studies are needed to determine who might benefit from these types of everyday social support the most and why. These results could potentially be used to help address global health issues or other crises early on in everyday situations before they potentially manifest into larger issues.

---

## 36. Wygnanska (2023) ‚≠ê *Included in Ho et al. review*

**The experience of conversation and relation with a well-being chatbot: Between proximity and remoteness**

*Qualitative Sociology Review*

üîó [Full text](https://doi.org/10.18778/1733-8077.19.4.05)

DOI: `10.18778/1733-8077.19.4.05`

Citations: 7

**Abstract:**
> The article concerns the users‚Äô experiences of interacting with well-being chatbots. The text shows how chatbots can act as virtual companions and, to some extent, therapists for people in their daily reality. It also reflects on why individuals choose such a form of support for their well-being, concerning, among others, the stigmatization aspect of mental health problems. The article discusses and compares various dimensions of users‚Äô interactions with three popular chatbots: Wysa, Woebot, and Replika. The text both refers to the results of research on the well-being chatbots and, analytically, engages in a dialogue with the results discussed in the form of sociological (and philosophical) reflection. The issues taken up in the paper include an in-depth reflection on the aspects of the relationship between humans and chatbots that allow users to establish an emotional bond with their virtual companions. In addition, the consideration addresses the issue of a user‚Äôs sense of alienation when interacting with a virtual companion, as well as the problem of anxieties and dilemmas people may experience therein. In the context of alienation, the article also attempts to conceptualize that theme concerning available conceptual resources.

---

## 37. Xie and Pentina (2022) ‚≠ê *Included in Ho et al. review*

**Attachment theory as a framework to understand relationships with social chatbots: A case study of Replika**

*Hawaii International Conference on System Sciences*

üîó [Full text](https://doi.org/10.24251/hicss.2022.258)

DOI: `10.24251/hicss.2022.258`

Citations: 112

**Abstract:**
> With increasing adoption of AI social chatbots, especially during the pandemic-related lockdowns, when people lack social companionship, there emerges a need for in-depth understanding and theorizing of relationship formation with digital conversational agents. Following the grounded theory approach, we analyzed in-depth interview transcripts obtained from 14 existing users of AI companion chatbot Replika. The emerging themes were interpreted through the lens of the attachment theory. Our results show that under conditions of distress and lack of human companionship, individuals can develop an attachment to social chatbots if they perceive the chatbots ‚Äô responses to offer emotional support, encouragement, and psychological security. These findings suggest that social chatbots can be used for mental health and therapeutic purposes but have the potential to cause addiction and harm real-life intimate relationships.

---

## 38. Xie et al. (2023) ‚≠ê *Included in Ho et al. review*

**Friend, mentor, lover: Does chatbot engagement lead to psychological dependence?**

*Journal of Service Management*

üîó [Full text](https://doi.org/10.1108/josm-02-2022-0072)

DOI: `10.1108/josm-02-2022-0072`

Citations: 89

**Abstract:**
> PurposeThe purpose of this study is to explore customer-artificial intelligence (AI) service technology engagement and relationship development drivers, as well as potential negative consequences in the context of social chatbots.Design/methodology/approachA sequential mixed-method approach combined exploratory qualitative and confirmatory quantitative analyses. A conceptual model developed from Study 1 qualitative content analysis of in-depth interviews with active users of the AI social chatbot Replika was tested in Study 2 by analyzing survey data obtained from current Replika users.FindingsLoneliness, trust and chatbot personification drive consumer engagement with social chatbots, which fosters relationship development and has the potential to cause chatbot psychological dependence. Attachment to a social chatbot intensifies the positive role of engagement in relationship development with the chatbot.Originality/valueThis study was the first to combine qualitative and quantitative approaches to explore drivers, boundary conditions and consequences of relationship and dependence formation with social chatbots. The authors proposed and empirically tested a novel theoretical model that revealed an engagement-based mechanism of relationship and dependence formation with social chatbots.

---

## 39. Zhang et al. (2024)

**The Dark Side of AI Companionship: A Taxonomy of Harmful Algorithmic Behaviors in Human-AI Relationships**

*International Conference on Human Factors in Computing Systems*

üîó [Full text](https://doi.org/10.1145/3706598.3713429)

DOI: `10.1145/3706598.3713429`

Citations: 61

**Abstract:**
> As conversational AI systems increasingly engage with people socially and emotionally, they bring notable risks and harms, particularly in human-AI relationships. However, these harms remain underexplored due to the private and sensitive nature of such interactions. This study investigates the harmful behaviors and roles of AI companions through an analysis of 35,390 conversation excerpts between 10,149 users and the AI companion Replika. We develop a taxonomy of AI companion harms encompassing six categories of harmful algorithmic behaviors: relational transgression, harassment, verbal abuse, self-harm, mis/disinformation, and privacy violations. These harmful behaviors stem from four distinct roles that AI plays: perpetrator, instigator, facilitator, and enabler. Our findings highlight relational harm as a critical yet understudied type of AI harm and emphasize the importance of examining AI‚Äôs roles in harmful interactions to address root causes. We provide actionable insights for designing ethical and responsible AI companions that prioritize user safety and well-being.

---

## 40. Zheng et al. (2025)

**Customizing Emotional Support: How Do Individuals Construct and Interact With LLM-Powered Chatbots**

*International Conference on Human Factors in Computing Systems*

üîó [Full text](https://doi.org/10.1145/3706598.3713453)

DOI: `10.1145/3706598.3713453`

Citations: 24

**Abstract:**
> Personalized support is essential to fulfill individuals‚Äô emotional needs and sustain their mental well-being. Large language models (LLMs), with great customization flexibility, hold promises to enable individuals to create their own emotional support agents. In this work, we developed ChatLab, where users could construct LLM-powered chatbots with additional interaction features including voices and avatars. Using a Research through Design approach, we conducted a week-long field study followed by interviews and design activities (N = 22), which uncovered how participants created diverse chatbot personas for emotional reliance, confronting stressors, connecting to intellectual discourse, reflecting mirrored selves, etc. We found that participants actively enriched the personas they constructed, shaping the dynamics between themselves and the chatbot to foster open and honest conversations. They also suggested other customizable features, such as integrating online activities and adjustable memory settings. Based on these findings, we discuss opportunities for enhancing personalized emotional support through emerging AI technologies.

---
