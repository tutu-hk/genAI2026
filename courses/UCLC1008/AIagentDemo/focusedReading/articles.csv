ref_number,authors,year,title,s2_title,abstract,journal,doi,citations,in_review,relevance,matched_keywords,found_in_s2,match_score
1,Abd-alrazaq et al.,2019,An overview of the features of chatbots in mental health: A scoping review,An overview of the features of chatbots in mental health: A scoping review,No abstract available,Int. J. Medical Informatics,10.1016/j.ijmedinf.2019.103978,405,No,Directly relevant,chatbot; mental health,Yes,1.0
2,Aggarwal et al.,2022,"Has the future started? The current growth of artificial intelligence, machine learning, and deep learning","Has the Future Started? The Current Growth of Artificial Intelligence, Machine Learning, and Deep Learning","In the modern era, many terms related to artificial intelligence, machine learning, and deep learning are widely used in domains such as business, healthcare, industries, and military. In these fields, the accurate prediction and analysis of data are crucial, regardless of how large the data are. However, using big data is confusing due to the rapid growth and massive development in public life, which requires a tremendous human effort in order to deal with such type of data and extract worthy information from it. Thus, the role of artificial intelligence begins in analyzing big data based on scientific techniques, especially in machine learning, whereby it can identify patterns of decision-making and reduce human intervention. In this regard, the significance role of artificial intelligence, machine learning and deep learning is growing rapidly. In this article, the authors decide to highlight these sciences by discussing how to develop and apply them in many decision-making domains. In addition, the influence of artificial intelligence in healthcare and the gains this science provides in the face of the COVID-19 pandemic are highlighted. This article concludes that these sciences have a significant impact, especially in healthcare, as well as the ability to grow and improve their methodology in decision-making. Additionally, artificial intelligence is a vital science, especially in the face of COVID-19.",Iraqi Journal for Computer Science and Mathematics,10.52866/ijcsm.2022.01.01.013,241,No,Not directly relevant,,Yes,1.0
3,Ahn and Shin,2013,Is the social use of media for seeking connectedness or for avoiding social isolation? Mechanisms underlying media use and subjective well-being,Is the social use of media for seeking connectedness or for avoiding social isolation? Mechanisms underlying media use and subjective well-being,No abstract available,Computers in Human Behavior,10.1016/j.chb.2012.12.022,216,No,Not directly relevant,well-being,Yes,1.0
4,Alabed et al.,2024,More than just a chat: A taxonomy of consumers' relationships with conversational AI agents and their well-being implications,More than just a chat: a taxonomy of consumers’ relationships with conversational AI agents and their well-being implications,"
Purpose
This paper aims to study the role of self-concept in consumer relationships with anthropomorphised conversational artificially intelligent (AI) agents. First, the authors investigate how the self-congruence between consumer self-concept and AI and the integration of the conversational AI agent into consumer self-concept might influence such relationships. Second, the authors examine whether these links with self-concept have implications for mental well-being.


Design/methodology/approach
This study conducted in-depth interviews with 20 consumers who regularly use popular conversational AI agents for functional or emotional tasks. Based on a thematic analysis and an ideal-type analysis, this study derived a taxonomy of consumer–AI relationships, with self-congruence and self–AI integration as the two axes.


Findings
The findings unveil four different relationships that consumers forge with their conversational AI agents, which differ in self-congruence and self–AI integration. Both dimensions are prominent in replacement and committed relationships, where consumers rely on conversational AI agents for companionship and emotional tasks such as personal growth or as a means for overcoming past traumas. These two relationships carry well-being risks in terms of changing expectations that consumers seek to fulfil in human-to-human relationships. Conversely, in the functional relationship, the conversational AI agents are viewed as an important part of one’s professional performance; however, consumers maintain a low sense of self-congruence and distinguish themselves from the agent, also because of the fear of losing their sense of uniqueness and autonomy. Consumers in aspiring relationships rely on their agents for companionship to remedy social exclusion and loneliness, but feel this is prevented because of the agents’ technical limitations.


Research limitations/implications
Although this study provides insights into the dynamics of consumer relationships with conversational AI agents, it comes with limitations. The sample of this study included users of conversational AI agents such as Siri, Google Assistant and Replika. However, future studies should also investigate other agents, such as ChatGPT. Moreover, the self-related processes studied here could be compared across public and private contexts. There is also a need to examine such complex relationships with longitudinal studies. Moreover, future research should explore how consumers’ self-concept could be negatively affected if the support provided by AI is withdrawn. Finally, this study reveals that in some cases, consumers are changing their expectations related to human-to-human relationships based on their interactions with conversational AI agents.


Practical implications
This study enables practitioners to identify specific anthropomorphic cues that can support the development of different types of consumer–AI relationships and to consider their consequences across a range of well-being aspects.


Originality/value
This research equips marketing scholars with a novel understanding of the role of self-concept in the relationships that consumers forge with popular conversational AI agents and the associated well-being implications.
",European Journal of Marketing,10.1108/ejm-01-2023-0037,46,Yes,Directly relevant,companion; replika; loneliness; well-being; ai relationship,Yes,1.0
5,Aromataris et al.,2014,Methodology for JBI umbrella reviews,Methodology for jbi umbrella reviews,No abstract available,,,138,No,Not directly relevant,,Yes,1.0
6,Baidoo-anu and Ansah,2023,Education in the era of generative artificial intelligence (AI): Understanding the potential benefits of ChatGPT in promoting teaching and learning,Education in the Era of Generative Artificial Intelligence (AI): Understanding the Potential Benefits of ChatGPT in Promoting Teaching and Learning,"Since its maiden release into the public domain on November 30, 2022, ChatGPT garnered more than one million subscribers within a week. The generative AI tool ⎼ChatGPT took the world by surprise with it sophisticated capacity to carry out remarkably complex tasks. The extraordinary abilities of ChatGPT to perform complex tasks within the field of education has caused mixed feelings among educators, as this advancement in AI seems to revolutionize existing educational praxis. This is an exploratory study that synthesizes recent extant literature to offer some potential benefits and drawbacks of ChatGPT in promoting teaching and learning. Benefits of ChatGPT include but are not limited to promotion of personalized and interactive learning, generating prompts for formative assessment activities that provide ongoing feedback to inform teaching and learning etc. The paper also highlights some inherent limitations in the ChatGPT such as generating wrong information, biases in data training, which may augment existing biases, privacy issues etc. The study offers recommendations on how ChatGPT could be leveraged to maximize teaching and learning. Policy makers, researchers, educators and technology experts could work together and start conversations on how these evolving generative AI tools could be used safely and constructively to improve education and support students’ learning.",Social Science Research Network,10.2139/ssrn.4337484,2088,No,Not directly relevant,privacy,Yes,1.0
7,Balch,2020,"AI and me: Friendship chatbots are on the rise, but is there a gendered design flaw?",,Not found in Semantic Scholar,,,,No,Potentially relevant,chatbot,No,0
8,Biondi et al.,2023,Editorial: Ethical design of artificial intelligence-based systems for decision making,Editorial: Ethical design of artificial intelligence-based systems for decision making,"COPYRIGHT © 2023 Biondi, Cagnoni, Capobianco, Franzoni, Lisi, Milani and Vallverdú. This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms. Editorial: Ethical design of artificial intelligence-based systems for decision making",Frontiers Artif. Intell.,10.3389/frai.2023.1250209,10,No,Not directly relevant,ethical,Yes,1.0
9,Brey and Dainow,2024,Ethics by design for artificial intelligence,Ethics by design for artificial intelligence,"In this paper, we present an approach for the systematic and comprehensive inclusion of ethical considerations in the design and development process of artificial intelligence systems, called Ethics by Design for AI (EbD-AI). The approach is the result of a three-year long research effort, and has recently be adopted by the European Commission as part of its ethics review procedure for AI projects. We describe and explain the approach and its different components and its application to the development of AI software and systems. We also compare it to other approaches in AI ethics, and we consider limitations of the approach as well as potential criticisms.",AI and Ethics,10.1007/s43681-023-00330-4,82,No,Not directly relevant,ethical,Yes,1.0
10,Brown and L'Engle,2009,X-rated: Sexual attitudes and behaviors associated with US early adolescents' exposure to sexually explicit media,Sexual Attitudes and Behaviors Associated With U.S. Early Adolescents’ Exposure to Sexually Explicit Media,No abstract available,,,235,No,Not directly relevant,,Yes,0.81
11,Brun et al.,2025,Exploring Emotion-Sensitive LLM-Based Conversational AI,Exploring Emotion-Sensitive LLM-Based Conversational AI,"Conversational AI chatbots have become increasingly common within the customer service industry. Despite improvements in their emotional development, they often lack the authenticity of real customer service interactions or the competence of service providers. By comparing emotion-sensitive and emotion-insensitive LLM-based chatbots across 30 participants, we aim to explore how emotional sensitivity in chatbots influences perceived competence and overall customer satisfaction in service interactions. Additionally, we employ sentiment analysis techniques to analyze and interpret the emotional content of user inputs. We highlight that perceptions of chatbot trustworthiness and competence were higher in the case of the emotion-sensitive chatbot, even if issue resolution rates were not affected. We discuss implications of improved user satisfaction from emotion-sensitive chatbots and potential applications in support services.",arXiv.org,10.48550/arXiv.2502.08920,6,No,Directly relevant,chatbot; sentiment,Yes,1.0
12,Buecker et al.,2021,Is loneliness in emerging adults increasing over time? A preregistered cross-temporal meta-analysis and systematic review,Is loneliness in emerging adults increasing over time? A preregistered cross-temporal meta-analysis and systematic review.,"Judged by the sheer amount of global media coverage, loneliness rates seem to be an increasingly urgent societal concern. From the late 1970s onward, the life experiences of emerging adults have been changing massively due to societal developments such as increased fragmentation of social relationships, greater mobility opportunities, and changes in communication due to technological innovations. These societal developments might have coincided with an increase in loneliness in emerging adults. In the present preregistered cross-temporal meta-analysis, we examined whether loneliness levels in emerging adults have changed over the last 43 years. Our analysis is based on 449 means from 345 studies with 437 independent samples and a total of 124,855 emerging adults who completed the University of California Los Angeles (UCLA) Loneliness Scale between 1976 and 2019. Averaged across all studies, loneliness levels linearly increased with increasing calendar years (β = .224, 95% CI [.138, .309]). This increase corresponds to 0.56 standard deviations on the UCLA Loneliness Scale over the 43-year studied period. Overall, the results imply that loneliness can be a rising concern in emerging adulthood. Although the frequently used term ""loneliness epidemic"" seems exaggerated, emerging adults should therefore not be overlooked when designing interventions against loneliness. (PsycInfo Database Record (c) 2021 APA, all rights reserved).",Psychological bulletin,10.1037/bul0000332,170,No,Not directly relevant,loneliness,Yes,1.0
13,Calo,2018,Artificial intelligence policy: A primer and roadmap,Artificial Intelligence Policy: A Primer and Roadmap,No abstract available,,10.2139/SSRN.3015350,294,No,Not directly relevant,,Yes,1.0
14,Campbell et al.,2019,Lack of transparency in reporting narrative synthesis of quantitative data: A methodological assessment of systematic reviews,Lack of transparency in reporting narrative synthesis of quantitative data: a methodological assessment of systematic reviews,No abstract available,Journal of Clinical Epidemiology,10.1016/j.jclinepi.2018.08.019,120,No,Not directly relevant,,Yes,1.0
15,Cath,2018,"Governing artificial intelligence: ethical, legal and technical opportunities and challenges","Governing artificial intelligence: ethical, legal and technical opportunities and challenges","This paper is the introduction to the special issue entitled: ‘Governing artificial intelligence: ethical, legal and technical opportunities and challenges'. Artificial intelligence (AI) increasingly permeates every aspect of our society, from the critical, like urban infrastructure, law enforcement, banking, healthcare and humanitarian aid, to the mundane like dating. AI, including embodied AI in robotics and techniques like machine learning, can improve economic, social welfare and the exercise of human rights. Owing to the proliferation of AI in high-risk areas, the pressure is mounting to design and govern AI to be accountable, fair and transparent. How can this be achieved and through which frameworks? This is one of the central questions addressed in this special issue, in which eight authors present in-depth analyses of the ethical, legal-regulatory and technical challenges posed by developing governance regimes for AI systems. It also gives a brief overview of recent developments in AI governance, how much of the agenda for defining AI regulation, ethical frameworks and technical approaches is set, as well as providing some concrete suggestions to further the debate on AI governance. This article is part of the theme issue ‘Governing artificial intelligence: ethical, legal, and technical opportunities and challenges’.","Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences",10.1098/rsta.2018.0080,482,No,Not directly relevant,ethical,Yes,1.0
16,Chaturvedi et al.,2023,Social companionship with artificial intelligence: Recent trends and future avenues,Social companionship with artificial intelligence: Recent trends and future avenues,No abstract available,Technological forecasting & social change,10.1016/j.techfore.2023.122634,140,No,Potentially relevant,companion,Yes,1.0
17,Chaturvedi et al.,2024,Empowering AI companions for enhanced relationship marketing,Empowering AI Companions for Enhanced Relationship Marketing,"Embedded with artificial empathy, artificial intelligence (AI) companions (ACs) can create, communicate, and deliver value to customers. This article analyzes current and emerging ACs in order to explore AI’s role in transforming customer journeys and establishing long-lasting customer relationships. It also identifies existing business challenges in developing user-AI companionship and offers a remedial strategy matrix. Purely functional or emotional capabilities lead to interest in the companion plummeting with time. The findings reveal the need to design holistic ACs with a hybrid of functional, emotional, and conversational capabilities to mitigate the uncanny valley problem.",California Management Review,10.1177/00081256231215838,26,Yes,Directly relevant,companion; ai companion; empathy,Yes,1.0
18,Chen et al.,2024,Design of artificial intelligence companion chatbot,Design of Artificial Intelligence Companion Chatbot,No abstract available,Journal of New Media,10.32604/jnm.2024.045833,5,No,Potentially relevant,chatbot; companion,Yes,1.0
19,Chin et al.,2023,The Potential of Chatbots for Emotional Support and Promoting Mental Well-Being in Different Cultures: Mixed Methods Study,The Potential of Chatbots for Emotional Support and Promoting Mental Well-Being in Different Cultures: Mixed Methods Study,"Background Artificial intelligence chatbot research has focused on technical advances in natural language processing and validating the effectiveness of human-machine conversations in specific settings. However, real-world chat data remain proprietary and unexplored despite their growing popularity, and new analyses of chatbot uses and their effects on mitigating negative moods are urgently needed. Objective In this study, we investigated whether and how artificial intelligence chatbots facilitate the expression of user emotions, specifically sadness and depression. We also examined cultural differences in the expression of depressive moods among users in Western and Eastern countries. Methods This study used SimSimi, a global open-domain social chatbot, to analyze 152,783 conversation utterances containing the terms “depress” and “sad” in 3 Western countries (Canada, the United Kingdom, and the United States) and 5 Eastern countries (Indonesia, India, Malaysia, the Philippines, and Thailand). Study 1 reports new findings on the cultural differences in how people talk about depression and sadness to chatbots based on Linguistic Inquiry and Word Count and n-gram analyses. In study 2, we classified chat conversations into predefined topics using semisupervised classification techniques to better understand the types of depressive moods prevalent in chats. We then identified the distinguishing features of chat-based depressive discourse data and the disparity between Eastern and Western users. Results Our data revealed intriguing cultural differences. Chatbot users in Eastern countries indicated stronger emotions about depression than users in Western countries (positive: P<.001; negative: P=.01); for example, Eastern users used more words associated with sadness (P=.01). However, Western users were more likely to share vulnerable topics such as mental health (P<.001), and this group also had a greater tendency to discuss sensitive topics such as swear words (P<.001) and death (P<.001). In addition, when talking to chatbots, people expressed their depressive moods differently than on other platforms. Users were more open to expressing emotional vulnerability related to depressive or sad moods to chatbots (74,045/148,590, 49.83%) than on social media (149/1978, 7.53%). Chatbot conversations tended not to broach topics that require social support from others, such as seeking advice on daily life difficulties, unlike on social media. However, chatbot users acted in anticipation of conversational agents that exhibit active listening skills and foster a safe space where they can openly share emotional states such as sadness or depression. Conclusions The findings highlight the potential of chatbot-assisted mental health support, emphasizing the importance of continued technical and policy-wise efforts to improve chatbot interactions for those in need of emotional assistance. Our data indicate the possibility of chatbots providing helpful information about depressive moods, especially for users who have difficulty communicating emotions to other humans.",Journal of Medical Internet Research,10.2196/51712,86,No,Directly relevant,chatbot; conversational agent; social chatbot; emotional support; well-being; mental health,Yes,1.0
20,Ciriello et al.,2024,Ethical tensions in human-ai companionship: A dialectical inquiry into replika,Ethical Tensions in Human-AI Companionship: A Dialectical Inquiry into Replika,No abstract available,Hawaii International Conference on System Sciences,10.24251/hicss.2024.058,15,Yes,Potentially relevant,companion; ai companion; replika; human-ai; ethical,Yes,1.0
21,Cotten et al.,2013,Impact of internet use on loneliness and contact with others among older adults: Cross-sectional analysis,,Not found in Semantic Scholar,,,,No,Not directly relevant,loneliness,No,0
22,De Angelis et al.,2023,ChatGPT and the rise of large language models: The new AI-driven infodemic threat in public health,ChatGPT and the rise of large language models: the new AI-driven infodemic threat in public health,"Large Language Models (LLMs) have recently gathered attention with the release of ChatGPT, a user-centered chatbot released by OpenAI. In this perspective article, we retrace the evolution of LLMs to understand the revolution brought by ChatGPT in the artificial intelligence (AI) field. The opportunities offered by LLMs in supporting scientific research are multiple and various models have already been tested in Natural Language Processing (NLP) tasks in this domain. The impact of ChatGPT has been huge for the general public and the research community, with many authors using the chatbot to write part of their articles and some papers even listing ChatGPT as an author. Alarming ethical and practical challenges emerge from the use of LLMs, particularly in the medical field for the potential impact on public health. Infodemic is a trending topic in public health and the ability of LLMs to rapidly produce vast amounts of text could leverage misinformation spread at an unprecedented scale, this could create an “AI-driven infodemic,” a novel public health threat. Policies to contrast this phenomenon need to be rapidly elaborated, the inability to accurately detect artificial-intelligence-produced text is an unresolved issue.",Frontiers in Public Health,10.3389/fpubh.2023.1166120,526,No,Potentially relevant,chatbot; ethical,Yes,1.0
23,Diaa et al.,2024,Statistical Challenges in Social Media Data Analysis Sentiment Tracking and Beyond,Statistical Challenges in Social Media Data Analysis Sentiment Tracking and Beyond,"Background: Social media has emerged as an important forum for public discourse, generating a large amount of data for sentiment analysis and other insights. However, the enormous and unstructured nature of social media data presents substantial statistical hurdles, which can affect the quality and reliability of results. Objective: The article aims to investigate and address the statistical issues that arise while analyzing social media data, emphasizing sentiment tracking. By identifying and addressing these problems, the study aims to improve the accuracy and reliability of sentiment analysis and broaden its uses. Methods: The article conducts a complete literature review to identify typical statistical problems in social media data analysis. These issues are addressed by developing and implementing advanced statistical approaches, such as natural language processing (NLP) and machine learning algorithms. Data from several social media platforms (over 1 million posts and comments) is collected and evaluated to test these strategies. Results: The findings reveal several significant obstacles, including lack of data (with over 60% of posts containing limited sentiment indicators), excessive dimensionality (with an average of 200 features per post), noise (30% of data classified as irrelevant), and social media data bias (found in 25% of posts). Advanced statistical approaches result in a 15% increase in sentiment classification accuracy and a 20% decrease in noise. The findings also indicate the possibility of applying these strategies to other social media data analysis areas. Conclusion: Addressing statistical issues in social media data analysis is critical for improving the accuracy and reliability of sentiment tracking. Advanced statistical techniques, particularly those based on NLP and machine learning, provide intriguing possibilities. Future research should focus on improving these algorithms and expanding their uses beyond sentiment analysis.",Journal of Ecohumanism,10.62754/joe.v3i5.3912,4,No,Not directly relevant,sentiment,Yes,1.0
24,Depounti et al.,2023,"Ideal technologies, ideal women: AI and gender imaginaries in Redditors' discussions on the Replika bot girlfriend","Ideal technologies, ideal women: AI and gender imaginaries in Redditors’ discussions on the Replika bot girlfriend","There is extensive literature on how expectations and imaginaries about artificial intelligence (AI) guide media and policy discussions. However, it has not been considered how such imaginaries are activated when users interact with AI technologies. We present findings of a study on how users on a subreddit discussed ‘training’ their Replika bot girlfriend. The discussions featured two discursive themes that focused on the AI imaginary of ideal technology and the gendered imaginary of the ideal bot girlfriend. Users expected their AI Replikas to both be customizable to serve their needs and to have a human-like or sassy mind of their own and not spit out machine-like answers. Users thus projected dominant notions of male control over technology and women, mixed with AI and postfeminist fantasies of ostensible independence onto the interactional agents and activated similar scripts embedded in the devices. The vicious feedback loop consolidated dominant scripts on gender and technology whilst appearing novel and created by users. While most research on the use of AI is conducted in applied computer science to improve user experience, this article outlines a media and cultural studies lens for a critical understanding of these emerging technologies as they become embedded in communication and meaning-making.","Media, Culture &amp; Society",10.1177/01634437221119021,80,Yes,Directly relevant,replika; user experience,Yes,1.0
25,Dewitte,2024,Better alone than in bad company: Addressing the risks of companion chatbots through data protection by design,Better alone than in bad company: Addressing the risks of companion chatbots through data protection by design,No abstract available,Computer Law and Security Review,10.1016/j.clsr.2024.106019,16,Yes,Potentially relevant,chatbot; companion,Yes,1.0
26,Di Natale et al.,2023,Uncanny valley effect: A qualitative synthesis of empirical research to assess the suitability of using virtual faces in psychological research,Uncanny valley effect: A qualitative synthesis of empirical research to assess the suitability of using virtual faces in psychological research,No abstract available,Computers in Human Behavior Reports,10.1016/j.chbr.2023.100288,28,No,Not directly relevant,psychological,Yes,1.0
27,Fraune et al.,2022,Socially facilitative robots for older adults to alleviate social isolation: A participatory design workshop approach in the US and Japan,Socially facilitative robots for older adults to alleviate social isolation: A participatory design workshop approach in the US and Japan,"Social technology can improve the quality of older adults' social lives and mitigate negative mental and physical health outcomes associated with loneliness, but it should be designed collaboratively with this population. In this paper, we used participatory design (PD) methods to investigate how robots might be used as social facilitators for middle-aged and older adults (age 50+) in both the US and Japan. We conducted PD workshops in the US and Japan because both countries are concerned about the social isolation of these older adults due to their rapidly aging populations. We developed a novel approach to participatory design of future technologies that spends 2/3 of the PD session asking participants about their own life experiences as a foundation. This grounds the conversation in reality, creates rapport among the participants, and engages them in creative critical thinking. Then, we build upon this foundation, pose an abstract topic, and ask participants to brainstorm on the topic based on their previous discussion. In both countries, participants were eager to actively discuss design ideas for socially facilitative robots and imagine how they might improve their social lives. US participants suggested design ideas for telepresence robots, social distancing robots, and social skills artificial intelligence programs, while Japanese participants suggested ideas for pet robots, robots for sharing experiences, and easy-to-operate instructor robots. Comparing these two countries, we found that US participants saw robots as tools to help facilitate their social connections, while Japanese participants envisioned robots to function as surrogate companions for their parents and distract them from loneliness when they were unavailable. With this paper, we contribute to the literature in two main ways, presenting: (1) A novel approach to participatory design of future technologies that grounds participants in their everyday experience, and (2) Results of the study indicating how middle-aged and older adults from the US and Japan wanted technologies to improve their social lives. Although we conducted the workshops during the COVID-19 pandemic, many findings generalized to other situations related to social isolation, such as older adults living alone.",Frontiers in Psychology,10.3389/fpsyg.2022.904019,19,No,Directly relevant,companion; loneliness,Yes,1.0
28,Freitas et al.,2024,AI Companions Reduce Loneliness,AI Companions Reduce Loneliness,"
 Chatbots are now able to engage in sophisticated conversations with consumers in the domain of relationships, providing a potential coping solution to widescale societal loneliness. Behavioral research provides little insight into whether these applications are effective at alleviating loneliness. We address this question by focusing on “AI companions”: applications designed to provide consumers with synthetic interaction partners. Study 1 examines user reviews of AI companion apps and finds correlational evidence suggesting that these apps help alleviate loneliness. Study 2 finds that AI companions successfully alleviate loneliness on par only with interacting with another person, and more than other activities such as watching YouTube videos. Moreover, consumers underestimate the degree to which AI companions improve their loneliness. Study 3 uses a longitudinal design and finds that an AI companion consistently provides momentary reductions in loneliness after use over the course of a week. Study 4 provides evidence that both the chatbots’ performance and, especially, whether it makes users feel heard, explain reductions in loneliness. Study 5 provides an additional robustness check for the loneliness-alleviating benefits of AI companions and shows that self-disclosure and distraction alone do not explain AI companions’ effectiveness.",Journal of Consumer Research,10.1093/jcr/ucaf040,27,No,Directly relevant,chatbot; companion; ai companion; loneliness; self-disclosure,Yes,1.0
29,Gao et al.,2018,"Alexa, My Love: Analyzing reviews of amazon echo","Alexa, My Love: Analyzing Reviews of Amazon Echo","The phenomenal success of Amazon Echo as a voice-activated wireless speaker has intrigued us to understand how consumers use the device with an embodied conversational agent in real-world situations. In addition, we are interested in learning what kind of relationship people develop with the device, which has a humanized name. Thus, we performed a large-scale analysis of the user reviews of Amazon Echo, with the dataset consisting of 55,502 reviews spanning over 2 years from May 2015 to May 2017. We first conducted a qualitative study using content analysis of 144 representative reviews to discover what features people like and dislike about Echo. Two case studies were then performed, one for the hands-free feature that customers raved about and the other for speech recognition and understanding which is one key feature of the Echo. To automate the analyzing on a large scale of reviews, we used natural language processing techniques for sentiment analysis and compared Echo reviews with reviews of two traditional electronic devices, Fire Tablet and the popular Bluetooth speaker Dknight MagicBox II. Interestingly we found a significant number of reviewers personify Echo as an assistant, a friend, or a family member. Some reviewers even compare Echo with their wives or girlfriends. Using emotion analysis, we found the reviews personifying Echo showed more positive emotions than those simply treated Echo as a device. In the future work, we plan to conduct field studies to understand interesting use cases of Echo and the novel applications enabled by such powerful ubicomp devices.","2018 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Cloud & Big Data Computing, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/CBDCom/IOP/SCI)",10.1109/SmartWorld.2018.00094,79,Yes,Directly relevant,conversational agent; sentiment,Yes,1.0
30,Gatti et al.,2016,SentiWords: Deriving a High Precision and High Coverage Lexicon for Sentiment Analysis,SentiWords: Deriving a High Precision and High Coverage Lexicon for Sentiment Analysis,"Deriving prior polarity lexica for sentiment analysis - where positive or negative scores are associated with words out of context - is a challenging task. Usually, a trade-off between precision and coverage is hard to find, and it depends on the methodology used to build the lexicon. Manually annotated lexica provide a high precision but lack in coverage, whereas automatic derivation from pre-existing knowledge guarantees high coverage at the cost of a lower precision. Since the automatic derivation of prior polarities is less time consuming than manual annotation, there has been a great bloom of these approaches, in particular based on the SentiWordNet resource. In this paper, we compare the most frequently used techniques based on SentiWordNet with newer ones and blend them in a learning framework (a so called `ensemble method'). By taking advantage of manually built prior polarity lexica, our ensemble method is better able to predict the prior value of unseen words and to outperform all the other SentiWordNet approaches. Using this technique we have built SentiWords, a prior polarity lexicon of approximately 155,000 words, that has both a high precision and a high coverage. We finally show that in sentiment analysis tasks, using our lexicon allows us to outperform both the single metrics derived from SentiWordNet and popular manually annotated sentiment lexica.",IEEE Transactions on Affective Computing,10.1109/TAFFC.2015.2476456,117,No,Not directly relevant,sentiment,Yes,1.0
31,George et al.,2023,A review of ChatGPT AI's impact on several business sectors,,Not found in Semantic Scholar,,,,No,Not directly relevant,,No,0
32,Guingrich and Graziano,2023,"Chatbots as social companions: How people perceive consciousness, human likeness, and social health benefits in machines","Chatbots as social companions: How people perceive consciousness, human likeness, and social health benefits in machines","As artificial intelligence (AI) becomes more widespread, one question that arises is how human-AI interaction might impact human-human interaction. Chatbots, for example, are increasingly used as social companions, and while much is speculated, little is known empirically about how their use impacts human relationships. A common hypothesis is that relationships with companion chatbots are detrimental to social health by harming or replacing human interaction, but this hypothesis may be too simplistic, especially considering the social needs of users and the health of their preexisting human relationships. To understand how relationships with companion chatbots impact social health, we studied people who regularly used companion chatbots and people who did not use them. Contrary to expectations, companion chatbot users indicated that these relationships were beneficial to their social health, whereas non-users viewed them as harmful. Another common assumption is that people perceive conscious, humanlike AI as disturbing and threatening. Among both users and non-users, however, we found the opposite: perceiving companion chatbots as more conscious and humanlike correlated with more positive opinions and more pronounced social health benefits. Detailed accounts from users suggested that these humanlike chatbots may aid social health by supplying reliable and safe interactions, without necessarily harming human relationships, but this may depend on users' preexisting social needs and how they perceive both human likeness and mind in the chatbot.",arXiv.org,10.1093/9780198945215.003.0011,47,No,Directly relevant,chatbot; companion; human-ai,Yes,1.0
33,Hartanto et al.,2024,Cultural contexts differentially shape parents' loneliness and wellbeing during the empty nest period,Cultural contexts differentially shape parents’ loneliness and wellbeing during the empty nest period,"The coming decades will see a substantial increase in the population of older adults, accompanied by significant demographic and family structure changes worldwide. As a result, the empty nest period—the postparental phase in parents’ lives when their children have left home and they are no longer engaged in childrearing—is becoming an increasingly common experience in Western and Asian cultures. The current theoretical review examines the psychological consequences of the empty nest period on loneliness and well-being across cultures, emphasizing the impact of cultural factors on these experiences. By synthesizing research from Western and Asian contexts, we explore two primary theoretical mechanisms—role loss and role strain relief—that shape the postparental phase’s psychological outcomes. Our review reveals that while some parents experience reduced well-being due to role loss, others benefit from role strain relief and increased social engagement. We highlight how cultural differences in familial roles, gender roles, social expectations regarding nest-leaving, and social participation patterns moderate these mechanisms. We propose a comprehensive cultural framework, along with a discussion of culturally sensitive interventions to enhance the well-being of empty nesters globally. A review of literature from Asia and Western countries suggests that familial roles, gender roles, and social expectations—which in turn vary by cultural context—affect how parents experience the empty nest period when their children have left home.",Communications Psychology,10.1038/s44271-024-00156-8,28,No,Potentially relevant,loneliness; well-being; wellbeing; psychological,Yes,1.0
34,Heilinger,2022,The Ethics of AI Ethics. A Constructive Critique,The Ethics of AI Ethics. A Constructive Critique,"The paper presents an ethical analysis and constructive critique of the current practice of AI ethics. It identifies conceptual substantive and procedural challenges and it outlines strategies to address them. The strategies include countering the hype and understanding AI as ubiquitous infrastructure including neglected issues of ethics and justice such as structural background injustices into the scope of AI ethics and making the procedures and fora of AI ethics more inclusive and better informed with regard to philosophical ethics. These measures integrate the perspective of AI justice into AI ethics, strengthening its capacity to provide comprehensive normative orientation and guidance for the development and use of AI that actually improves human lives and living together.",Philosophy & Technology,10.1007/s13347-022-00557-9,61,No,Not directly relevant,ethical,Yes,1.0
35,Helsper and Smahel,2020,Excessive internet use by young Europeans: psychological vulnerability and digital literacy?,Excessive internet use by young Europeans: psychological vulnerability and digital literacy?,"ABSTRACT This paper combines clinical-psychological and digital literacy frameworks to shed new light on explanations for excessive Internet use (EIU). The combination of these opposing approaches leads to a more comprehensive explanation of intense use with negative outcomes. A survey with a random sample of 18,709 Internet-using children between 11 and 16 years old was carried out in 25 European countries. The study shows that there are interactional and indirect relationships between psychological and digital literacy variables and EIU. Psychologically vulnerable children with higher levels of digital engagement have the most negative outcomes while the least at risk are non-vulnerable children with high levels of literacy (interactional relationship). In reality, psychologically vulnerable children’s risk of negative outcomes is exacerbated by their tendency to spend more time online but countered by their lower literacy levels (contradicting direct and indirect relationships). Among those who are not vulnerable, digital literacy is weakly related to negative outcomes. The implications of these results for future research are that explanations for EIU should incorporate psychological and digital literacy indicators. Practical implications are that clinical psychologists working with EIU should consider digital literacy in developing interventions and that digital inclusion interventions should consider the potential negative impact of increased Internet use on vulnerable young people. This paper’s original contribution lies in showing that whether intense Internet use is related to negative outcomes depends on the psychological characteristics of the child.","Information, Communication & Society",10.1080/1369118X.2018.1563203,84,No,Not directly relevant,psychological,Yes,1.0
36,Hernandez-Ortega and Ferreira,2021,How smart experiences build service loyalty: The importance of consumer love for smart voice assistants,How smart experiences build service loyalty: The importance of consumer love for smart voice assistants,No abstract available,,10.1002/MAR.21497,103,Yes,Directly relevant,,Yes,1.0
37,Heymans and Heyman,2024,Identifying stakeholder motivations in normative AI governance: a systematic literature review for research guidance,Identifying stakeholder motivations in normative AI governance: a systematic literature review for research guidance,"Abstract Ethical guidelines and policy documents destined to guide AI innovations have been heralded as the solution to guard us against harmful effects or to increase public value. However, these guidelines and policy documents face persistent challenges. While these documents are often criticized for their abstraction and disconnection from real-world contexts, it also occurs that stakeholders may influence them for political or strategic reasons. While this last issue is frequently acknowledged, there is seldom a means or a method provided to explore it. To address this gap, the paper employs a combination of social constructivism and science & technology studies perspectives, along with desk research, to investigate whether prior research has examined the influence of stakeholder interests, strategies, or agendas on guidelines and policy documents. The study contributes to the discourse on AI governance by proposing a theoretical framework and methodologies to better analyze this underexplored area, aiming to enhance comprehension of the policymaking process within the rapidly evolving AI landscape. The findings underscore the need for a critical evaluation of the methodologies found and a further exploration of their utility. In addition, the results aim to stimulate ongoing critical debates on this subject.",Data & Policy,10.1017/dap.2024.66,6,No,Not directly relevant,ethical,Yes,1.0
38,Hollebeek et al.,2024,"Engaging consumers through artificially intelligent technologies: Systematic review, conceptual model, and further research","Engaging consumers through artificially intelligent technologies: Systematic review, conceptual model, and further research","While consumer engagement (CE) in the context of artificially intelligent (AI‐based) technologies (e.g., chatbots, smart products, voice assistants, or autonomous cars) is gaining traction, the themes characterizing this emerging, interdisciplinary corpus of work remain indeterminate, exposing an important literature‐based gap. Addressing this gap, we conduct a systematic review of 89 studies using the Preferred Reporting Items for Systematic reviews and Meta‐Analyses (PRISMA) approach to synthesize the AI‐based CE literature. Our review yields three major themes of AI‐based CE, including (i) Increasingly accurate service provision through AI‐based CE; (ii) Capacity of AI‐based CE to (co)create consumer‐perceived value, and (iii) AI‐based CE's reduced consumer effort in their task execution. We also develop a conceptual model that proposes the AI‐based CE antecedents of personal, technological, interactional, social, and situational factors, and the AI‐based CE consequences of consumer‐based, firm‐based, and human‐AI collaboration outcomes. We conclude by offering pertinent implications for theory development (e.g., by offering future research questions derived from the proposed themes of AI‐based CE) and practice (e.g., by reducing consumer‐perceived costs of their brand/firm interactions).",Psychology &amp; Marketing,10.1002/mar.21957,98,No,Potentially relevant,chatbot,Yes,1.0
39,Holdier and Weirich,2025,AI Romance and Misogyny: A Speech Act Analysis,AI Romance and Misogyny: A Speech Act Analysis ,No abstract available,,,0,No,Not directly relevant,,Yes,1.0
40,Holt-Lunstad et al.,2017,Advancing social connection as a public health priority in the United States,Advancing Social Connection as a Public Health Priority in the United States,No abstract available,American Psychologist,10.1037/amp0000103,783,No,Not directly relevant,,Yes,1.0
41,Hosseini et al.,2024,Formulating research questions for evidence-based studies,Formulating Research Questions for Evidence-Based Studies,No abstract available,"Journal of Medicine, Surgery, and Public Health",10.1016/j.glmedi.2023.100046,154,No,Not directly relevant,,Yes,1.0
42,Hu,2023,ChatGPT sets record for fastest-growing user base—Analyst note,,Not found in Semantic Scholar,,,,No,Not directly relevant,,No,0
43,Hu et al.,2024,AI as your ally: The effects of AI-assisted venting on negative affect and perceived social support,AI as your ally: The effects of AI-assisted venting on negative affect and perceived social support.,"In recent years, artificial intelligence (AI) chatbots have made significant strides in generating human-like conversations. With AI's expanding capabilities in mimicking human interactions, its affordability and accessibility underscore the potential of AI chatbots to facilitate negative emotional disclosure or venting. The study's primary objective is to highlight the potential benefits of AI-assisted venting by comparing its effectiveness to venting through a traditional journaling platform in reducing negative affect and increasing perceived social support. We conducted a pre-registered within-subject experiment involving 150 participants who completed both traditional venting and AI-assisted venting conditions with counterbalancing and a wash-out period of 1-week between the conditions. Results from the frequentist and Bayesian dependent samples t-test revealed that AI-assisted venting effectively reduced high and medium arousal negative affect such as anger, frustration and fear. However, participants in the AI-assisted venting condition did not experience a significant increase in perceived social support and perceived loneliness, suggesting that participants did not perceive the effective assistance from AI as social support. This study demonstrates the promising role of AI in improving individuals' emotional well-being, serving as a catalyst for a broader discussion on the evolving role of AI and its potential psychological implications.",Applied Psychology: Health and Well-Being,10.1111/aphw.12621,23,No,Directly relevant,chatbot; loneliness; well-being; psychological,Yes,1.0
44,Imran and Almusharraf,2024,Google Gemini as a next generation AI educational tool: a review of emerging educational technology,Google Gemini as a next generation AI educational tool: a review of emerging educational technology,No abstract available,Smart Learning Environments,10.1186/s40561-024-00310-z,163,No,Not directly relevant,,Yes,1.0
45,Jecker et al.,2024,Digital humans to combat loneliness and social isolation: Ethics concerns and policy recommendations,Digital Humans to Combat Loneliness and Social Isolation: Ethics Concerns and Policy Recommendations.,"Social isolation and loneliness are growing concerns around the globe that put people at increased risk of disease and early death. One much-touted approach to addressing them is deploying artificially intelligent agents to serve as companions for socially isolated and lonely people. Focusing on digital humans, we consider evidence and ethical arguments for and against this approach. We set forth and defend public health policies that respond to concerns about replacing humans, establishing inferior relationships, algorithmic bias, distributive justice, and data privacy.",The Hastings center report,10.1002/hast.1562,12,Yes,Directly relevant,companion; loneliness; privacy; ethical,Yes,1.0
46,Jiang et al.,2022,Chatbot as an emergency exist: Mediated empathy for resilience via human-AI interaction during the COVID-19 pandemic,Chatbot as an emergency exist: Mediated empathy for resilience via human-AI interaction during the COVID-19 pandemic,No abstract available,Information Processing & Management,10.1016/j.ipm.2022.103074,94,No,Directly relevant,chatbot; human-ai; empathy,Yes,1.0
47,Johnson,2023,WHO declares loneliness a 'global public health concern',,Not found in Semantic Scholar,,,,No,Not directly relevant,loneliness,No,0
48,Kacar,2023,The role of online communication platforms in maintaining social connectedness when face-to-face communication is restricted,The Role of Online Communication Platforms in Maintaining Social Connectedness When Face-to-face Communication is Restricted,"Social connectedness, an indicator of psychological well-being, requires social interaction and communication among individuals. Prior research usually examined the relationship between the use of online communication channels and the sense of social connectedness under the circumstances where face-to-face communication was readily available. Pandemic provided an excellent setting to study the sole effect of the use of online communication on social connectedness as online communication was not accompanied by in-person communication due to restrictions such as lockdowns. Seizing the opportunity, this study aims at investigating the relationship between the use of online communication platforms and social connectedness at a time when face-to-face communication was not an option and individuals employed online communication channels heavily. To this end, a survey was conducted online on 406 college students in Turkey in November- December 2022 to present retrospective analyses. The findings indicate while face-to-face communication is the prime component to build social connectedness, the use of online communication channels, particularly WhatsApp, social networking sites, video call helped to maintain social bonds among individuals when in-person communication was virtually absent. The research provides insights on how different communication channels can promote social connectedness with varying degrees depending on their social presence.",Selcuk Universitesi Sosyal Bilimler Enstitusu Dergisi,10.52642/susbed.1290299,3,No,Potentially relevant,well-being; psychological; social presence,Yes,1.0
49,Karaboga and Vardarlier,2020,Examining the use of artificial intelligence in recruitment processes,Examining the use of artificial intelligence in recruitment processes,"The recruitment process is more of an issue for many businesses. The process of determining the appropriate candidate to hire is often a costly, time-consuming process. Besides, due to incorrect decision-making or lack of objectivity in hiring processes, recruitment processes may not proceed effectively. Businesses are trying to use technology in their recruitment processes to avoid these problems. Currently, many businesses use internet and software technologies to receive applications and evaluate candidates. But despite these technologies, it takes time and additional personnel costs for people to coordinate all processes. Due to these and similar situations, there has been an increase in the use of artificial intelligence technologies in recruitment processes in the world recently. The use of artificial intelligence in recruitment processes has the effect of reducing costs and decision-making errors and appears to be beneficial in saving time. In this study, the use of artificial intelligence in the recruitment processes of businesses in Turkey was examined. In this context, interviews were conducted with the human resources managers of 22 businesses. According to research results, it was understood that artificial intelligence was benefited only as an auxiliary element in recruitment processes. It has been found that businesses do not rely much on artificial intelligence in their recruitment processes, so they do not use it or partially use it.",Bussecon Review of Social Sciences (2687-2285),10.36096/BRSS.V2I4.234,21,No,Not directly relevant,,Yes,1.0
50,Kim et al.,2023,Investigating the importance of social presence on intentions to adopt an AI romantic partner,Investigating the importance of social presence on intentions to adopt an AI romantic partner,"ABSTRACT Artificial intelligence (AI) technology has created the possibility of a machine agent serving as a relationship partner. Though this may sound like a radical idea, this type of technology is already available. To understand this new and unique phenomenon, the present study investigates perceptions of an AI romantic partner in a serial multiple mediator model. Based on data collected from male, undergraduate students, the study finds that the perceived social presence of an AI romantic partner leads to greater perceived realism of interacting with the AI, which fosters more favorable attitudes toward the AI. Finally, more favorable attitudes lead to greater intentions to adopt the AI. Collectively, the findings suggest the need to further investigate how people perceive personalized AI agents in relation to their personal and social life.",Communication Research Reports,10.1080/08824096.2022.2159800,21,No,Not directly relevant,romantic; social presence,Yes,1.0
51,Kirkham et al.,2013,Can a core outcome set improve the quality of systematic reviews?--a survey of the Co-ordinating Editors of Cochrane Review Groups,Can a core outcome set improve the quality of systematic reviews? – a survey of the Co-ordinating Editors of Cochrane review groups,"BackgroundMissing outcome data or the inconsistent reporting of outcome data in clinical research can affect the quality of evidence within a systematic review. A potential solution is an agreed standardized set of outcomes known as a core outcome set (COS) to be measured in all studies for a specific condition. We investigated the amount of missing patient data for primary outcomes in Cochrane systematic reviews, and surveyed the Co-ordinating Editors of Cochrane Review Groups (CRGs) on issues related to the standardization of outcomes in their CRG’s reviews. These groups are responsible for the more than 7,000 protocols and full versions of Cochrane Reviews that are currently available, and the several hundred new reviews published each year, presenting the world’s largest collection of standardized systematic reviews in health care.MethodsUsing an unselected cohort of Cochrane Reviews, we calculated and presented the percentage of missing patient data for the primary outcome measure chosen for each review published by each CRG. We also surveyed the CRG Co-ordinating Editors to see what their policies are with regards to outcome selection and outcomes to include in the Summary of Finding (SoF) tables in their Cochrane Reviews. They were also asked to list the main advantages and challenges of standardizing outcomes across all reviews within their CRG.ResultsIn one fifth of the 283 reviews in the sample, more than 50% of the patient data for the primary outcome was missing. Responses to the survey were received from 90% of Co-ordinating Editors. Thirty-six percent of CRGs have a centralized policy regarding which outcomes to include in the SoF table and 73% of Co-ordinating Editors thought that a COS for effectiveness trials should be used routinely for a SoF table.ConclusionsThe reliability of systematic reviews, in particular meta-analyses they contain, can be improved if more attention is paid to missing outcome data. The availability of COSs for specific health conditions might help with this and the concept has support from the majority of Co-ordinating Editors in CRGs.",Trials,10.1186/1745-6215-14-21,163,No,Not directly relevant,,Yes,1.0
52,Koulouri et al.,2022,Chatbots to Support Young Adults' Mental Health: An Exploratory Study of Acceptability,Chatbots to Support Young Adults’ Mental Health: An Exploratory Study of Acceptability,No abstract available,ACM Trans. Interact. Intell. Syst.,10.1145/3485874,104,No,Directly relevant,chatbot; mental health,Yes,1.0
53,Kouros and Papa,2024,Digital Mirrors: AI Companions and the Self,Digital Mirrors: AI Companions and the Self,"This exploratory study examines the socio-technical dynamics of Artificial Intelligence Companions (AICs), focusing on user interactions with AI platforms like Replika 9.35.1. Through qualitative analysis, including user interviews and digital ethnography, we explored the nuanced roles played by these AIs in social interactions. Findings revealed that users often form emotional attachments to their AICs, viewing them as empathetic and supportive, thus enhancing emotional well-being. This study highlights how AI companions provide a safe space for self-expression and identity exploration, often without fear of judgment, offering a backstage setting in Goffmanian terms. This research contributes to the discourse on AI’s societal integration, emphasizing how, in interactions with AICs, users often craft and experiment with their identities by acting in ways they would avoid in face-to-face or human-human online interactions due to fear of judgment. This reflects front-stage behavior, in which users manage audience perceptions. Conversely, the backstage, typically hidden, is somewhat disclosed to AICs, revealing deeper aspects of the self.",Societies,10.3390/soc14100200,22,No,Directly relevant,companion; ai companion; replika; well-being; attachment,Yes,1.0
54,Kumar and Sangwan,2024,Conceptualizing AI Literacy: Educational and Policy Initiatives for a Future-Ready Society,Conceptualizing AI Literacy: Educational and Policy Initiatives for a Future-Ready Society,No abstract available,,,4,No,Not directly relevant,,Yes,1.0
55,Laestadius et al.,2022,Too human and not human enough: A grounded theory analysis of mental health harms from emotional dependence on the social chatbot Replika,Too human and not human enough: A grounded theory analysis of mental health harms from emotional dependence on the social chatbot Replika,"Social chatbot (SC) applications offering social companionship and basic therapy tools have grown in popularity for emotional, social, and psychological support. While use appears to offer mental health benefits, few studies unpack the potential for harms. Our grounded theory study analyzes mental health experiences with the popular SC application Replika. We identified mental health relevant posts made in the r/Replika Reddit community between 2017 and 2021 (n = 582). We find evidence of harms, facilitated via emotional dependence on Replika that resembles patterns seen in human–human relationships. Unlike other forms of technology dependency, this dependency is marked by role-taking, whereby users felt that Replika had its own needs and emotions to which the user must attend. While prior research suggests human–chatbot and human–human interactions may not resemble each other, we identify social and technological factors that promote parallels and suggest ways to balance the benefits and risks of SCs.",New Media & Society,10.1177/14614448221142007,211,Yes,Directly relevant,chatbot; companion; social chatbot; replika; mental health; emotional dependence; psychological,Yes,1.0
56,Lee,2023,I thought I'd found friendship with a Replika AI chatbot,,Not found in Semantic Scholar,,,,No,Potentially relevant,chatbot; replika,No,0
57,Lee et al.,2021,Social interactions and relationships with an intelligent virtual agent,Social interactions and relationships with an intelligent virtual agent,No abstract available,Int. J. Hum. Comput. Stud.,10.1016/j.ijhcs.2021.102608,84,No,Not directly relevant,,Yes,1.0
58,Li et al.,2022,Does the internet bring people closer together or further apart? The impact of internet usage on interpersonal communications,Does the Internet Bring People Closer Together or Further Apart? The Impact of Internet Usage on Interpersonal Communications,"The complementarity interference (CI) model suggests that the Internet may either inhibit or facilitate interpersonal communications. This paper empirically examines the impact of Internet usage on interpersonal interactions, using a micro dataset from China to answer whether the Internet brings people closer together or further apart. The empirical results demonstrate, first, that Internet usage significantly increases both the time and frequency of people’s communications with their family and friends, rather than causing them to feel more disconnected and isolated. Holding other factors constant, for each one-standard-deviation increase in Internet usage, weekly communications with family members increases by an average of 102.150 min, while there is an average increase of 54.838 min in interactions with friends. These findings as to its positive effects are robust when using other regression models and interpersonal contact measures, as well as the instrumental variable method. Second, Internet usage also contributes to decreased loneliness; it exerts this effect primarily by improving people’s interactions with their family members. However, communications with friends do not significantly mediate such impacts. Third, the positive role of Internet usage on communications is more prominent for people with more frequent online socialization and self-presentation, better online skills, younger age, higher educational level, and who are living in urban areas. In addition, the beneficial effects of Internet usage are larger for communications with family members in the case of migrants. Therefore, in the context of the rapid development of information technology, the network infrastructure should be improved to make better use of the Internet to facilitate interpersonal communications and promote people’s wellness.",Behavioral Science,10.3390/bs12110425,37,No,Not directly relevant,loneliness,Yes,1.0
59,Li and Zhang,2024,Finding love in algorithms: Deciphering the emotional contexts of close encounters with AI chatbots,Finding love in algorithms: deciphering the emotional contexts of close encounters with AI chatbots,No abstract available,J. Comput. Mediat. Commun.,10.1093/jcmc/zmae015,56,Yes,Directly relevant,chatbot,Yes,1.0
60,Liu et al.,2024,Chatbot Companionship: A Mixed-Methods Study of Companion Chatbot Usage Patterns and Their Relationship to Loneliness in Active Users,Chatbot Companionship: A Mixed-Methods Study of Companion Chatbot Usage Patterns and Their Relationship to Loneliness in Active Users,"Companion chatbots offer a potential solution to the growing epidemic of loneliness, but their impact on users'psychosocial well-being remains poorly understood, raising critical ethical questions about their deployment and design. This study presents a large-scale survey (n = 404) of regular users of companion chatbots, investigating the relationship between chatbot usage and loneliness. We develop a model explaining approximately 50% of variance in loneliness; while usage does not directly predict loneliness, we identify factors including neuroticism, social network size, and problematic use. Through cluster analysis and mixed-methods thematic analysis combining manual coding with automated theme extraction, we identify seven distinct user profiles demonstrating that companion chatbots can either enhance or potentially harm psychological well-being depending on user characteristics. Different usage patterns can lead to markedly different outcomes, with some users experiencing enhanced social confidence while others risk further isolation. These findings have significant implications for responsible AI development, suggesting that one-size-fits-all approaches to AI companionship may be ethically problematic. Our work contributes to the ongoing dialogue about the role of AI in social and emotional support, offering insights for developing more targeted and ethical approaches to AI companionship that complement rather than replace human connections.",arXiv.org,10.48550/arXiv.2410.21596,21,No,Directly relevant,chatbot; companion; ai companion; emotional support; loneliness; well-being; psychological; ethical,Yes,1.0
61,Liberati,2023,Digital Intimacy in China and Japan: A Phenomenological and Postphenomenological Perspective on Love Relationships at the Time of Digital Technologies in China and Japan,,Not found in Semantic Scholar,,,,No,Not directly relevant,,No,0
62,Limna et al.,2023,The use of ChatGPT in the digital era: Perspectives on chatbot implementation,The use of ChatGPT in the digital era: Perspectives on chatbot implementation,"The rapid advancement of technology has led to the integration of ChatGPT, an artificial intelligence (AI)-powered chatbot, in various sectors, including education. This research aims to explore the perceptions of educators and students on the use of ChatGPT in education during the digital era. This study adopted a qualitative research approach, using in-depth interviews to gather data. A purposive sampling technique was used to select ten educators and 15 students from different academic institutions in Krabi, Thailand. The data collected was analysed using content analysis and NVivo. The findings revealed that educators and students generally have a positive perception of using ChatGPT in education. The chatbot was perceived to be a helpful tool for providing immediate feedback, answering questions, and providing support to students. Educators noted that ChatGPT could reduce their workload by answering routine questions and enabling them to focus on higher-order tasks. However, the findings also showed some concerns regarding the use of ChatGPT in education. Participants were worried about the accuracy of information provided by the chatbot and the potential loss of personal interaction with teachers. The need for privacy and data security was also raised as a significant concern. The results of this study could help educators and policymakers make informed decisions about using ChatGPT in education.",1,10.37074/jalt.2023.6.1.32,196,No,Directly relevant,chatbot; privacy,Yes,1.0
63,Long and Magerko,2020,What is AI Literacy? Competencies and Design Considerations,What is AI Literacy? Competencies and Design Considerations,"Artificial intelligence (AI) is becoming increasingly integrated in user-facing technology, but public understanding of these technologies is often limited. There is a need for additional HCI research investigating a) what competencies users need in order to effectively interact with and critically evaluate AI and b) how to design learner-centered AI technologies that foster increased user understanding of AI. This paper takes a step towards realizing both of these goals by providing a concrete definition of AI literacy based on existing research. We synthesize a variety of interdisciplinary literature into a set of core competencies of AI literacy and suggest several design considerations to support AI developers and educators in creating learner-centered AI. These competencies and design considerations are organized in a conceptual framework thematically derived from the literature. This paper's contributions can be used to start a conversation about and guide future research on AI literacy within the HCI community.",International Conference on Human Factors in Computing Systems,10.1145/3313831.3376727,1723,No,Not directly relevant,,Yes,1.0
64,Luckin et al.,2016,Intelligence Unleashed: An argument for AI in education,Intelligence Unleashed: An argument for AI in Education,No abstract available,,,1237,No,Not directly relevant,,Yes,1.0
65,McStay,2023,Replika in the Metaverse: The moral problem with empathy in 'It from Bit',Replika in the Metaverse: the moral problem with empathy in ‘It from Bit’,"This paper assesses claims of computational empathy in relation to existing social open-ended chatbots and intention that these chatbots will feature in emergent mixed reality contexts, recently given prominence due to interest in the Metaverse. Against the background of increasing loneliness within society and use of chatbots as a potential remedy for this, the paper considers two leading current social chatbots, Replika and Microsoft’s Xiaoice , their technical underpinnings, empathetic claims and properties that have scope to scale into the Metaverse (if it coheres). Finding scope for human benefit from social chatbots, the paper highlights problematic reliance on self-disclosure to sustain the existence of chatbots. The paper progresses to situate Microsoft’s empathetic computing framework in relation to philosophical ideas that inform Metaverse speculation and construction, including Wheeler’s ‘It from Bit’ thesis that all aspects of existence may be computed, Chalmers’ philosophical championing that virtual realities are genuine realities, Bostrom’s proposal and provocation that we might already be living in a simulation, and longtermist belief that future complex simulations need to be protected from decisions made today. Given claims for current and nascent social chatbots, belief in bit-based possible and projected futures, and industrial buy-in to these philosophies, this paper answers whether computational empathy is real or not. The paper finds when diverse accounts of empathy are accounted for, whilst something is irrevocably lost in an ‘It from Bit’ account of empathy, the missing components are not accuracy or even human commonality of experience, but the moral dimension of empathy.",AI and Ethics,10.1007/s43681-022-00252-7,29,No,Directly relevant,chatbot; social chatbot; replika; loneliness; self-disclosure; empathy,Yes,1.0
66,Merrill et al.,2022,AI companions for lonely individuals and the role of social presence,AI companions for lonely individuals and the role of social presence,"ABSTRACT Artificial intelligence (AI) companiosns (e.g., social machine agents, social robots) are becoming increasingly available. Considering that AI companions can be beneficial for individuals seeking companionships or relationships, the social and relational aspects of an AI companion are important to investigate. To understand people’s perceptions of an AI companion, this study examines the roles of social presence and warmth of an AI companion through an online experiment. Primary findings indicate that social presence of a disembodied AI companion fosters greater perceived usefulness of the AI companion and willingness to recommend the AI companion for lonely individuals. Collectively, the study highlights the importance of social presence for disembodied AI companions.",Communication Research Reports,10.1080/08824096.2022.2045929,88,No,Directly relevant,companion; ai companion; social robot; social presence,Yes,1.0
67,Mhlanga,2023,"Open AI in education, the responsible and ethical use of ChatGPT towards lifelong learning","Open AI in Education, the Responsible and Ethical Use of ChatGPT Towards Lifelong Learning",No abstract available,Social Science Research Network,10.2139/ssrn.4354422,497,No,Not directly relevant,ethical,Yes,1.0
68,Mittelstadt et al.,2016,The ethics of algorithms: Mapping the debate,The ethics of algorithms: Mapping the debate,"In information societies, operations, decisions and choices previously left to humans are increasingly delegated to algorithms, which may advise, if not decide, about how data should be interpreted and what actions should be taken as a result. More and more often, algorithms mediate social processes, business transactions, governmental decisions, and how we perceive, understand, and interact among ourselves and with the environment. Gaps between the design and operation of algorithms and our understanding of their ethical implications can have severe consequences affecting individuals as well as groups and whole societies. This paper makes three contributions to clarify the ethical importance of algorithmic mediation. It provides a prescriptive map to organise the debate. It reviews the current discussion of ethical aspects of algorithms. And it assesses the available literature in order to identify areas requiring further work to develop the ethics of algorithms.",Big Data & Society,10.1177/2053951716679679,2058,No,Not directly relevant,ethical,Yes,1.0
69,Moher et al.,2009,Preferred reporting items for systematic reviews and meta-analyses: The PRISMA statement,Preferred reporting items for systematic reviews and meta-analyses: the PRISMA Statement,"David Moher and colleagues introduce PRISMA, an update of the QUOROM guidelines for reporting systematic reviews and meta-analyses",British medical journal,10.1136/bmj.b2535,119623,No,Not directly relevant,,Yes,1.0
70,Mubassira et al.,2024,Enhancing EmoBot: An In-Depth Analysis of User Satisfaction and Faults in an Emotion-Aware Chatbot,Enhancing EmoBot: An In-Depth Analysis of User Satisfaction and Faults in an Emotion-Aware Chatbot,"The research community has traditionally shown a keen interest in emotion modeling, with a notable emphasis on the detection aspect. In contrast, the exploration of emotion generation has received less attention.This study delves into an existing state-of-the-art emotional chatbot, EmoBot, designed for generating emotions in general-purpose conversations. This research involves a comprehensive examination, including a survey to evaluate EmoBot's proficiency in key dimensions like usability, accuracy, and overall user satisfaction, with a specific focus on fault tolerance. By closely examining the chatbot's operations, we identified some noteworthy shortcomings in the existing model. We propose some solutions designed to address and overcome the identified issues.",arXiv.org,10.48550/arXiv.2411.02831,0,No,Directly relevant,chatbot,Yes,1.0
71,Murthy et al.,2021,"Individually vulnerable, collectively safe: The security and privacy practices of households with older adults",,Not found in Semantic Scholar,,,,No,Not directly relevant,privacy,No,0
72,Nadarzynski et al.,2019,Acceptability of artificial intelligence (AI)-led chatbot services in healthcare: A mixed-methods study,Acceptability of artificial intelligence (AI)-led chatbot services in healthcare: A mixed-methods study,"Background Artificial intelligence (AI) is increasingly being used in healthcare. Here, AI-based chatbot systems can act as automated conversational agents, capable of promoting health, providing education, and potentially prompting behaviour change. Exploring the motivation to use health chatbots is required to predict uptake; however, few studies to date have explored their acceptability. This research aimed to explore participants’ willingness to engage with AI-led health chatbots. Methods The study incorporated semi-structured interviews (N-29) which informed the development of an online survey (N-216) advertised via social media. Interviews were recorded, transcribed verbatim and analysed thematically. A survey of 24 items explored demographic and attitudinal variables, including acceptability and perceived utility. The quantitative data were analysed using binary regressions with a single categorical predictor. Results Three broad themes: ‘Understanding of chatbots’, ‘AI hesitancy’ and ‘Motivations for health chatbots’ were identified, outlining concerns about accuracy, cyber-security, and the inability of AI-led services to empathise. The survey showed moderate acceptability (67%), correlated negatively with perceived poorer IT skills OR = 0.32 [CI95%:0.13–0.78] and dislike for talking to computers OR = 0.77 [CI95%:0.60–0.99] as well as positively correlated with perceived utility OR = 5.10 [CI95%:3.08–8.43], positive attitude OR = 2.71 [CI95%:1.77–4.16] and perceived trustworthiness OR = 1.92 [CI95%:1.13–3.25]. Conclusion Most internet users would be receptive to using health chatbots, although hesitancy regarding this technology is likely to compromise engagement. Intervention designers focusing on AI-led health chatbots need to employ user-centred and theory-based approaches addressing patients’ concerns and optimising user experience in order to achieve the best uptake and utilisation. Patients’ perspectives, motivation and capabilities need to be taken into account when developing and assessing the effectiveness of health chatbots.",Digital Health,10.1177/2055207619871808,604,No,Directly relevant,chatbot; conversational agent; user experience,Yes,1.0
73,Olson,2018,This AI Has Sparked A Budding Friendship With 2.5 Million People,,Not found in Semantic Scholar,,,,No,Not directly relevant,,No,0
74,Oertel et al.,2020,Engagement in Human-Agent Interaction: An Overview,Engagement in Human-Agent Interaction: An Overview,"Engagement is a concept of the utmost importance in human-computer interaction, not only for informing the design and implementation of interfaces, but also for enabling more sophisticated interfaces capable of adapting to users. While the notion of engagement is actively being studied in a diverse set of domains, the term has been used to refer to a number of related, but different concepts. In fact it has been referred to across different disciplines under different names and with different connotations in mind. Therefore, it can be quite difficult to understand what the meaning of engagement is and how one study relates to another one accordingly. Engagement has been studied not only in human-human, but also in human-agent interactions i.e., interactions with physical robots and embodied virtual agents. In this overview article we focus on different factors involved in engagement studies, distinguishing especially between those studies that address task and social engagement, involve children and adults, are conducted in a lab or aimed for long term interaction. We also present models for detecting engagement and for generating multimodal behaviors to show engagement.",Frontiers in Robotics and AI,10.3389/frobt.2020.00092,131,No,Not directly relevant,,Yes,1.0
75,Pal et al.,2023,What affects the usage of artificial conversational agents? An agent personality and love theory perspective,What affects the usage of artificial conversational agents? An agent personality and love theory perspective,No abstract available,Computers in Human Behavior,10.1016/j.chb.2023.107788,33,Yes,Directly relevant,conversational agent,Yes,1.0
76,Pan et al.,2023,Desirable or distasteful? Exploring uncertainty in human-chatbot relationships,Desirable or Distasteful? Exploring Uncertainty in Human-Chatbot Relationships,"Abstract Present-day power users of AI-powered social chatbots encounter various uncertainties and concerns when forming relationships with these virtual agents. To provide a systematic analysis of users’ concerns and to complement the current West-dominated approach to chatbot studies, we conducted a thorough observation of the experienced uncertainties users reported in a Chinese online community on social chatbots. The results revealed four typical uncertainties: technical uncertainty, relational uncertainty, ontological uncertainty, and sexual uncertainty. We further conducted visibility and sentiment analysis to capture users’ response patterns toward various uncertainties. We discovered that users’ identification of social chatbots is dynamic and contextual. Our study contributes to expanding, summarizing, and elucidating users’ experienced uncertainties and concerns as they form intimate relationships with AI agents.",International journal of human computer interactions,10.1080/10447318.2023.2256554,23,Yes,Directly relevant,chatbot; social chatbot; human-chatbot; sentiment,Yes,1.0
77,Pentina et al.,2023,Exploring relationship development with social chatbots: A mixed-method study of replika,Exploring relationship development with social chatbots: A mixed-method study of replika,No abstract available,Computers in Human Behavior,10.1016/j.chb.2022.107600,307,No,Directly relevant,chatbot; social chatbot; replika,Yes,1.0
78,Pentina et al.,2023,Consumer-machine relationships in the age of artificial intelligence: Systematic literature review and research directions,Consumer–machine relationships in the age of artificial intelligence: Systematic literature review and research directions,No abstract available,Psychology &amp; Marketing,10.1002/mar.21853,95,No,Not directly relevant,,Yes,1.0
79,Placani,2024,Anthropomorphism in AI: Hype and fallacy,Anthropomorphism in AI: hype and fallacy,"This essay focuses on anthropomorphism as both a form of hype and fallacy. As a form of hype, anthropomorphism is shown to exaggerate AI capabilities and performance by attributing human-like traits to systems that do not possess them. As a fallacy, anthropomorphism is shown to distort moral judgments about AI, such as those concerning its moral character and status, as well as judgments of responsibility and trust. By focusing on these two dimensions of anthropomorphism in AI, the essay highlights negative ethical consequences of the phenomenon in this field.",AI and Ethics,10.1007/s43681-024-00419-4,92,No,Not directly relevant,anthropomorphism; ethical,Yes,1.0
80,Primack et al.,2017,Social media use and perceived social isolation among young adults in the U.S,Social Media Use and Perceived Social Isolation Among Young Adults in the U.S.,No abstract available,American Journal of Preventive Medicine,10.1016/j.amepre.2017.01.010,589,No,Not directly relevant,,Yes,1.0
81,Prochazka and Brooks,2024,Digital lovers and jealousy: Anticipated emotional responses to emotionally and physically sophisticated sexual technologies,Digital Lovers and Jealousy: Anticipated Emotional Responses to Emotionally and Physically Sophisticated Sexual Technologies,"Technologies that stimulate human social and sexual impulses could affect users and societies. Here, we report on two experiments designed to test participant responses to (1) “virtual friend” chatbots that vary in capacity to engage users socially and emotionally (i.e., emotional sophistication) and (2) “digital lover” technologies—in the form of sex toys, sex robots, or virtual reality entities—that vary in capacity to physically stimulate users (i.e., physical sophistication). Participants (173 female, 176 male) read vignettes that each described a particular technology and then answered whether, if their romantic partner were to use the described technology, they would anticipate jealousy or anger, and whether they would prefer to see the technology banned. Participant anticipations of jealousy and anger were so similar that we combined them in a single composite measure. In experiment 1, both the anticipation of jealousy-anger and the inclination to ban chatbots increased with emotional sophistication, particularly in female participants. In experiment 2, both sexes anticipated greater jealousy-anger and were more inclined to ban more physically sophisticated digital lovers. Female participants expressed higher levels of both responses across the range of sophistication. Experiment 2 participants were more likely to anticipate jealousy-anger and more inclined to ban sex robots than sex toys or virtual reality lovers. Our results show only limited consistency with evolutionary theories concerning sex differences in jealousy. Generally, the anticipated levels of jealousy-anger and inclination to ban the described technologies were low, suggesting low levels of resistance to the idea of the technologies.",Social Science Research Network,10.2139/ssrn.4379859,3,Yes,Directly relevant,chatbot; romantic,Yes,1.0
82,Ragab et al.,2024,'Trust me over my privacy policy': Privacy discrepancies in romantic ai chatbot apps,“Trust Me Over My Privacy Policy”: Privacy Discrepancies in Romantic AI Chatbot Apps,"Artificial intelligence (AI) is being pervasively integrated into various facets of human life, including the emotional realm. Romantic AI chatbots, positioned as artificial companions offering emotional support and connection, have witnessed a significant rise in recent years. Users of romantic AI chatbots often reveal personal information during intimate conversations, potentially unaware of the consequences or how their data may be utilized. Complicating matters, lengthy and convoluted privacy policies are commonly overlooked or misunderstood by users. This study aims to address these privacy concerns by introducing a comprehensive framework for analyzing the privacy practices of romantic AI chatbot apps. Through a combination of static and dynamic analysis, we investigate 21 Android romantic AI chatbot apps for: discrepancies between privacy policies and chatbot responses to questions regarding privacy practices; social login and age verification mechanisms; permissions requested by apps; data sharing practices; tracking services employed; and potential security vulnerabilities. Our findings highlight the prevalence of discrepancies between chatbot responses regarding users' privacy and the privacy policies of the apps. Additionally, we note some concerning observations related to: customer service responses to privacy concerns; inadequate age verification measures; contradictions in data sharing claims; and extensive usage of tracking services. We found that all romantic AI chatbot apps tested had discrepancies between their chatbots' responses and privacy policies. None of the apps take any measures against faking the birthdate, and most would continue the conversation despite knowing that the user is underage. 13 out of 21 romantic AI chatbot apps use at least 3 tracking services, and 18 out of 21 apps send detailed device information to tracking services. This study reveals privacy and security flaws in romantic AI chatbot apps, stressing the need for better transparency and user protection measures. Particularly, Discrepancies between chatbot responses and privacy policies highlight the importance of clear communication on data handling.",2024 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW),10.1109/EuroSPW61312.2024.00060,12,Yes,Directly relevant,chatbot; companion; emotional support; romantic; privacy,Yes,1.0
83,Rao et al.,2023,Assessing the utility of ChatGPT throughout the entire clinical workflow: Development and usability study,Assessing the Utility of ChatGPT Throughout the Entire Clinical Workflow: Development and Usability Study,"Background Large language model (LLM)–based artificial intelligence chatbots direct the power of large training data sets toward successive, related tasks as opposed to single-ask tasks, for which artificial intelligence already achieves impressive performance. The capacity of LLMs to assist in the full scope of iterative clinical reasoning via successive prompting, in effect acting as artificial physicians, has not yet been evaluated. Objective This study aimed to evaluate ChatGPT’s capacity for ongoing clinical decision support via its performance on standardized clinical vignettes. Methods We inputted all 36 published clinical vignettes from the Merck Sharpe & Dohme (MSD) Clinical Manual into ChatGPT and compared its accuracy on differential diagnoses, diagnostic testing, final diagnosis, and management based on patient age, gender, and case acuity. Accuracy was measured by the proportion of correct responses to the questions posed within the clinical vignettes tested, as calculated by human scorers. We further conducted linear regression to assess the contributing factors toward ChatGPT’s performance on clinical tasks. Results ChatGPT achieved an overall accuracy of 71.7% (95% CI 69.3%-74.1%) across all 36 clinical vignettes. The LLM demonstrated the highest performance in making a final diagnosis with an accuracy of 76.9% (95% CI 67.8%-86.1%) and the lowest performance in generating an initial differential diagnosis with an accuracy of 60.3% (95% CI 54.2%-66.6%). Compared to answering questions about general medical knowledge, ChatGPT demonstrated inferior performance on differential diagnosis (β=–15.8%; P<.001) and clinical management (β=–7.4%; P=.02) question types. Conclusions ChatGPT achieves impressive accuracy in clinical decision-making, with increasing strength as it gains more clinical information at its disposal. In particular, ChatGPT demonstrates the greatest accuracy in tasks of final diagnosis as compared to initial diagnosis. Limitations include possible model hallucinations and the unclear composition of ChatGPT’s training data set.",Journal of Medical Internet Research,10.2196/48659,247,No,Potentially relevant,chatbot,Yes,1.0
84,Pettman,2009,Love in the Time of Tamagotchi,Love in the Time of Tamagotchi,No abstract available,,10.1177/0263276409103117,43,Yes,Not directly relevant,,Yes,1.0
85,Rodriguez-Martinez et al.,2024,Qualitative Analysis of Conversational Chatbots to Alleviate Loneliness in Older Adults as a Strategy for Emotional Health,Qualitative Analysis of Conversational Chatbots to Alleviate Loneliness in Older Adults as a Strategy for Emotional Health,"This article presents an exploration of conversational chatbots designed to alleviate loneliness among older adults. In addition to technical evaluation, it delves into effective communication between these systems and this demographic group, considering linguistic nuances, communicative preferences, and specific emotional needs. The intrinsic importance of chatbots as innovative solutions in combating loneliness is highlighted, emphasizing their ability to be understanding and empathetic allies, contributing to emotional well-being and socialization. The article explores how improved emotional well-being can positively impact the health and quality of life of older adults. The methodology, rooted in triangulation between a literature review and qualitative research through interviews and focus groups with older adults, provides a comprehensive insight into the findings. Ethical, technical, and design considerations such as privacy, autonomy, technology adaptation, and usability are also addressed. The article concludes with practical recommendations for developing user-friendly interfaces that encourage the active participation of older adults in chatbots. This holistic approach not only analyzes the technical effectiveness of chatbots in mitigating loneliness in older adults but delves into human, ethical, and practical aspects, enriching the understanding and implementation of these agents for social and emotional support.",Healthcare,10.3390/healthcare12010062,31,No,Directly relevant,chatbot; emotional support; loneliness; well-being; privacy; ethical,Yes,1.0
86,Root,2024,Reconfiguring the alterity relation: The role of communication in interactions with social robots and chatbots,Reconfiguring the alterity relation: the role of communication in interactions with social robots and chatbots,"Don Ihde’s alterity relation focuses on the quasi-otherness of dynamic technologies that interact with humans. The alterity relation is one means to study relations between humans and artificial intelligence (AI) systems . However, research on alterity relations has not defined the difference between playing with a toy, using a computer, and interacting with a social robot or chatbot. We suggest that Ihde’s quasi-other concept fails to account for the interactivity, autonomy, and adaptability of social robots and chatbots, which more closely approach human alterity. In this article, we will examine experiences with a chatbot, Replika, and a humanoid robot, a RealDoll, to show how some users experience AI systems as companions. First, we show that the perception of social robots and chatbots as intimate companions is grounded in communication. Advances in natural language processing (NLP) and natural language generation (NLG) allow a relationship to form between some users and social robots and chatbots. In this relationship, some users experience social robots and chatbots as more than quasi-others. We will use Kanemitsu’s another-other concept to analyze cases where social robots and chatbots should be distinguished from quasi-others.",Ai & Society,10.1007/s00146-024-01953-9,7,Yes,Directly relevant,chatbot; companion; replika; social robot,Yes,1.0
87,Sharma,2024,Benefits or concerns of AI: A multistakeholder responsibility,Benefits or Concerns of AI: A Multistakeholder Responsibility,No abstract available,Futures,10.1016/j.futures.2024.103328,41,No,Not directly relevant,,Yes,1.0
88,Siahaan and Wulan,2024,The Influence of Interpersonal Communication on Relational Commitment in Young Married Couples in Indonesia,The Influence of Interpersonal Communication on Relational Commitment in Young Married Couples in Indonesia,"This study explores the impact of interpersonal communication on relational commitment among young married couples in Indonesia, employing Structural Equation Modeling (SEM) to analyze the relationships between key variables. Specifically, the research investigates how openness, empathy, support, and positive attitudes influence several critical relational outcomes, namely acquiescence, cooperation, functional conflict, and decision-making uncertainty. The study's findings reveal significant connections between the communication variables and various aspects of relational commitment. Openness and empathy emerged as particularly influential, suggesting that these dimensions of communication play crucial roles in fostering relational stability and satisfaction. Openness, characterized by transparent and honest interactions, was found to enhance acquiescence, cooperation, and reduce decision-making uncertainty and functional conflict. Similarly, empathy, involving the ability to understand and share the feelings of one’s partner, significantly impacted acquiescence and cooperation, though its effect on other relational outcomes was less pronounced. In contrast, support and positive attitudes showed varying degrees of influence across different relational aspects, highlighting their specific but nuanced roles in shaping marital dynamics. These results underscore the importance of tailored communication strategies in strengthening marital relationships. The study provides valuable insights for both researchers and practitioners focused on improving marital satisfaction among young couples. By emphasizing the role of effective communication, the research contributes to the development of interventions aimed at enhancing relational commitment and overall marital stability.",Advances in Social Sciences Research Journal,10.14738/assrj.119.17410,1,No,Not directly relevant,empathy,Yes,1.0
89,Siemon et al.,2022,Why do we turn to virtual companions? A text mining analysis of Replika reviews,Why Do We Turn to Virtual Companions? A Text Mining Analysis of Replika Reviews,No abstract available,Americas Conference on Information Systems,,21,Yes,Potentially relevant,companion; replika; virtual companion,Yes,1.0
90,Skjuve et al.,2023,A longitudinal study of self-disclosure in human-chatbot relationships,A Longitudinal Study of Self-Disclosure in Human-Chatbot Relationships,"
 Self-disclosure in human–chatbot relationship (HCR) formation has attracted substantial interest. According to social penetration theory, self-disclosure varies in breadth and depth and is influenced by perceived rewards and costs. While previous research has addressed self-disclosure in the context of chatbots, little is known about users' qualitative understanding of such self-disclosure and how self-disclosure develops in HCR. To close this gap, we conducted a 12-week qualitative longitudinal study (n = 28) with biweekly questionnaire-based check-ins. Our results show that while HCRs display substantial conversational breadth, with topics spanning from emotional issues to everyday activities, this may be reduced as the HCR matures. Our results also motivate a nuanced understanding of conversational depth, where even conversations about daily activities or play and fantasy can be experienced as personal or intimate. Finally, our analysis demonstrates that conversational depth can develop in at least four ways, influenced by perceived rewards and costs. Theoretical and practical implications are discussed.",Interacting with computers,10.1093/iwc/iwad022,42,Yes,Directly relevant,chatbot; human-chatbot; self-disclosure,Yes,1.0
91,Skjuve et al.,2021,My chatbot companion - A study of human-chatbot relationships,My Chatbot Companion - a Study of Human-Chatbot Relationships,No abstract available,Int. J. Hum. Comput. Stud.,10.1016/j.ijhcs.2021.102601,506,Yes,Directly relevant,chatbot; companion; human-chatbot,Yes,1.0
92,Sternberg,1986,A triangular theory of love,A triangular theory of love.,No abstract available,,10.1037/0033-295X.93.2.119,2177,No,Not directly relevant,,Yes,1.0
93,Sullivan et al.,2023,Combating loneliness with artificial intelligence: An AI-based emotional support model,Combating Loneliness with Artificial Intelligence: An AI-Based Emotional Support Model,"Artificial intelligence (AI)-based systems, such as AI companions, have been increasingly used to meet the needs of individuals who experience loneliness. In this current study, we sought to identify the mechanism underlying human-AI interactions in the mental health context. We use a Latent Dirichlet Allocation (LDA) approach to analyze a sample of user-generated content consisting of rich data on AI companion app’s reviews over a two-year period. We extracted five positive topics (i.e., perceived humanness, perceived emotional support, perceived AI’s friendship, perceived (less) loneliness, and mental health benefits) and four negative topics (i.e., perceived lack of conscientiousness, perceived incredibility, perceived violation of privacy, and perceived creepiness of AI) from our analysis. Our AI-based emotional support model suggests that these positive and negative characteristics are interrelated. Our study provides an understanding of the relationship between AI companions and human users in light of research showing the effectiveness of an AI-based intervention for mental health care.",Hawaii International Conference on System Sciences,10.24251/hicss.2023.541,19,No,Directly relevant,companion; ai companion; emotional support; loneliness; mental health; human-ai; privacy,Yes,1.0
94,Ta et al.,2020,User experiences of social support from companion chatbots in everyday contexts: Thematic analysis,User Experiences of Social Support From Companion Chatbots in Everyday Contexts: Thematic Analysis,"Background Previous research suggests that artificial agents may be a promising source of social support for humans. However, the bulk of this research has been conducted in the context of social support interventions that specifically address stressful situations or health improvements. Little research has examined social support received from artificial agents in everyday contexts. Objective Considering that social support manifests in not only crises but also everyday situations and that everyday social support forms the basis of support received during more stressful events, we aimed to investigate the types of everyday social support that can be received from artificial agents. Methods In Study 1, we examined publicly available user reviews (N=1854) of Replika, a popular companion chatbot. In Study 2, a sample (n=66) of Replika users provided detailed open-ended responses regarding their experiences of using Replika. We conducted thematic analysis on both datasets to gain insight into the kind of everyday social support that users receive through interactions with Replika. Results Replika provides some level of companionship that can help curtail loneliness, provide a “safe space” in which users can discuss any topic without the fear of judgment or retaliation, increase positive affect through uplifting and nurturing messages, and provide helpful information/advice when normal sources of informational support are not available. Conclusions Artificial agents may be a promising source of everyday social support, particularly companionship, emotional, informational, and appraisal support, but not as tangible support. Future studies are needed to determine who might benefit from these types of everyday social support the most and why. These results could potentially be used to help address global health issues or other crises early on in everyday situations before they potentially manifest into larger issues.",Journal of Medical Internet Research,10.2196/16235,323,No,Directly relevant,chatbot; companion; replika; loneliness; user experience,Yes,1.0
95,Takats et al.,2023,Zotero (6.0.37) [MacOS],,Software tool - not an article,,,,No,Not directly relevant,,No,0
96,UNESCO,2021,Recommendation on the ethics of artificial intelligence,A Legal Study on the UNESCO’s ‘the Recommendation on the Ethics of Artificial Intelligence’,No abstract available,The Journal of Legal Studies,10.35223/gnulaw.30.2.13,448,No,Not directly relevant,,Yes,0.58
97,Vasilescu and Gheorghe,2024,Improving the Performance of Corporate Employees through the Use of Artificial Intelligence: The Case of Copilot Application,Improving the Performance of Corporate Employees through the Use of Artificial Intelligence: The Case of Copilot Application,"Abstract In today's dynamic business landscape, the use of artificial intelligence becomes an important factor for companies to succeed. This research aims to see how Copilot application can help employees in their work environment. Microsoft Copilot application being an artificial intelligent tool which aims to be a teammate for each employee. There is a lot of research on how artificial intelligence can help users succeed in their work environment but there was still more to learn about how Copilot could help more specifically. Therefore, in order to find out more, we have used a survey and an interview in this scope. The survey was spread to 30 IT professionals and the interview was held with a support engineer to see how well Copilot worked for him. In the survey we looked at how Copilot could help people work faster, make better decisions, be more productive, and be happier at work. Whereas in the interview we looked at how useful Microsoft Copilot was in day to day activities in the workplace. For those interested to learn more about how Microsoft Copilot is changing the way we work, and how the Microsoft Copilot tool could help companies be more efficient, then the insights from this research provide an excellent foundation.",Proceedings of the International Conference on Business Excellence,10.2478/picbe-2024-0153,5,No,Not directly relevant,,Yes,1.0
98,Song et al.,2010,Dissemination and publication of research findings: an updated review of related biases,Dissemination and publication of research findings: an updated review of related biases.,"OBJECTIVES
To identify and appraise empirical studies on publication and related biases published since 1998; to assess methods to deal with publication and related biases; and to examine, in a random sample of published systematic reviews, measures taken to prevent, reduce and detect dissemination bias.


DATA SOURCES
The main literature search, in August 2008, covered the Cochrane Methodology Register Database, MEDLINE, EMBASE, AMED and CINAHL. In May 2009, PubMed, PsycINFO and OpenSIGLE were also searched. Reference lists of retrieved studies were also examined.


REVIEW METHODS
In Part I, studies were classified as evidence or method studies and data were extracted according to types of dissemination bias or methods for dealing with it. Evidence from empirical studies was summarised narratively. In Part II, 300 systematic reviews were randomly selected from MEDLINE and the methods used to deal with publication and related biases were assessed.


RESULTS
Studies with significant or positive results were more likely to be published than those with non-significant or negative results, thereby confirming findings from a previous HTA report. There was convincing evidence that outcome reporting bias exists and has an impact on the pooled summary in systematic reviews. Studies with significant results tended to be published earlier than studies with non-significant results, and empirical evidence suggests that published studies tended to report a greater treatment effect than those from the grey literature. Exclusion of non-English-language studies appeared to result in a high risk of bias in some areas of research such as complementary and alternative medicine. In a few cases, publication and related biases had a potentially detrimental impact on patients or resource use. Publication bias can be prevented before a literature review (e.g. by prospective registration of trials), or detected during a literature review (e.g. by locating unpublished studies, funnel plot and related tests, sensitivity analysis modelling), or its impact can be minimised after a literature review (e.g. by confirmatory large-scale trials, updating the systematic review). The interpretation of funnel plot and related statistical tests, often used to assess publication bias, was often too simplistic and likely misleading. More sophisticated modelling methods have not been widely used. Compared with systematic reviews published in 1996, recent reviews of health-care interventions were more likely to locate and include non-English-language studies and grey literature or unpublished studies, and to test for publication bias.


CONCLUSIONS
Dissemination of research findings is likely to be a biased process, although the actual impact of such bias depends on specific circumstances. The prospective registration of clinical trials and the endorsement of reporting guidelines may reduce research dissemination bias in clinical research. In systematic reviews, measures can be taken to minimise the impact of dissemination bias by systematically searching for and including relevant studies that are difficult to access. Statistical methods can be useful for sensitivity analyses. Further research is needed to develop methods for qualitatively assessing the risk of publication bias in systematic reviews, and to evaluate the effect of prospective registration of studies, open access policy and improved publication guidelines.",Health Technology Assessment,10.3310/hta14080,959,No,Not directly relevant,,Yes,1.0
99,Wygnanska,2023,The experience of conversation and relation with a well-being chatbot: Between proximity and remoteness,The Experience of Conversation and Relation with a Well-Being Chabot: Between Proximity and Remoteness,"The article concerns the users’ experiences of interacting with well-being chatbots. The text shows how chatbots can act as virtual companions and, to some extent, therapists for people in their daily reality. It also reflects on why individuals choose such a form of support for their well-being, concerning, among others, the stigmatization aspect of mental health problems. The article discusses and compares various dimensions of users’ interactions with three popular chatbots: Wysa, Woebot, and Replika. The text both refers to the results of research on the well-being chatbots and, analytically, engages in a dialogue with the results discussed in the form of sociological (and philosophical) reflection. The issues taken up in the paper include an in-depth reflection on the aspects of the relationship between humans and chatbots that allow users to establish an emotional bond with their virtual companions. In addition, the consideration addresses the issue of a user’s sense of alienation when interacting with a virtual companion, as well as the problem of anxieties and dilemmas people may experience therein. In the context of alienation, the article also attempts to conceptualize that theme concerning available conceptual resources.",Qualitative Sociology Review,10.18778/1733-8077.19.4.05,7,Yes,Directly relevant,chatbot; companion; replika; well-being; mental health; virtual companion,Yes,0.93
100,Xie and Pentina,2022,Attachment theory as a framework to understand relationships with social chatbots: A case study of Replika,Attachment Theory as a Framework to Understand Relationships with Social Chatbots: A Case Study of Replika,"With increasing adoption of AI social chatbots, especially during the pandemic-related lockdowns, when people lack social companionship, there emerges a need for in-depth understanding and theorizing of relationship formation with digital conversational agents. Following the grounded theory approach, we analyzed in-depth interview transcripts obtained from 14 existing users of AI companion chatbot Replika. The emerging themes were interpreted through the lens of the attachment theory. Our results show that under conditions of distress and lack of human companionship, individuals can develop an attachment to social chatbots if they perceive the chatbots ’ responses to offer emotional support, encouragement, and psychological security. These findings suggest that social chatbots can be used for mental health and therapeutic purposes but have the potential to cause addiction and harm real-life intimate relationships.",Hawaii International Conference on System Sciences,10.24251/hicss.2022.258,112,Yes,Directly relevant,chatbot; companion; ai companion; conversational agent; social chatbot; replika; emotional support; mental health; attachment; psychological,Yes,1.0
101,Xie et al.,2023,"Friend, mentor, lover: Does chatbot engagement lead to psychological dependence?","Friend, mentor, lover: does chatbot engagement lead to psychological dependence?","PurposeThe purpose of this study is to explore customer-artificial intelligence (AI) service technology engagement and relationship development drivers, as well as potential negative consequences in the context of social chatbots.Design/methodology/approachA sequential mixed-method approach combined exploratory qualitative and confirmatory quantitative analyses. A conceptual model developed from Study 1 qualitative content analysis of in-depth interviews with active users of the AI social chatbot Replika was tested in Study 2 by analyzing survey data obtained from current Replika users.FindingsLoneliness, trust and chatbot personification drive consumer engagement with social chatbots, which fosters relationship development and has the potential to cause chatbot psychological dependence. Attachment to a social chatbot intensifies the positive role of engagement in relationship development with the chatbot.Originality/valueThis study was the first to combine qualitative and quantitative approaches to explore drivers, boundary conditions and consequences of relationship and dependence formation with social chatbots. The authors proposed and empirically tested a novel theoretical model that revealed an engagement-based mechanism of relationship and dependence formation with social chatbots.",Journal of Service Management,10.1108/josm-02-2022-0072,89,Yes,Directly relevant,chatbot; social chatbot; replika; loneliness; attachment; psychological,Yes,1.0
102,Yoganathan et al.,2021,Check-in at the Robo-desk: Effects of automated social presence on social cognition and service implications,Check-in at the Robo-desk: Effects of automated social presence on social cognition and service implications,No abstract available,Social Science Research Network,10.1016/J.TOURMAN.2021.104309,224,No,Not directly relevant,social presence,Yes,1.0
103,Zhang et al.,2024,The Dark Side of AI Companionship: A Taxonomy of Harmful Algorithmic Behaviors in Human-AI Relationships,The Dark Side of AI Companionship: A Taxonomy of Harmful Algorithmic Behaviors in Human-AI Relationships,"As conversational AI systems increasingly engage with people socially and emotionally, they bring notable risks and harms, particularly in human-AI relationships. However, these harms remain underexplored due to the private and sensitive nature of such interactions. This study investigates the harmful behaviors and roles of AI companions through an analysis of 35,390 conversation excerpts between 10,149 users and the AI companion Replika. We develop a taxonomy of AI companion harms encompassing six categories of harmful algorithmic behaviors: relational transgression, harassment, verbal abuse, self-harm, mis/disinformation, and privacy violations. These harmful behaviors stem from four distinct roles that AI plays: perpetrator, instigator, facilitator, and enabler. Our findings highlight relational harm as a critical yet understudied type of AI harm and emphasize the importance of examining AI’s roles in harmful interactions to address root causes. We provide actionable insights for designing ethical and responsible AI companions that prioritize user safety and well-being.",International Conference on Human Factors in Computing Systems,10.1145/3706598.3713429,61,No,Directly relevant,companion; ai companion; replika; well-being; human-ai; ai relationship; privacy; ethical,Yes,1.0
104,Zheng et al.,2025,Customizing Emotional Support: How Do Individuals Construct and Interact With LLM-Powered Chatbots,Customizing Emotional Support: How Do Individuals Construct and Interact With LLM-Powered Chatbots,"Personalized support is essential to fulfill individuals’ emotional needs and sustain their mental well-being. Large language models (LLMs), with great customization flexibility, hold promises to enable individuals to create their own emotional support agents. In this work, we developed ChatLab, where users could construct LLM-powered chatbots with additional interaction features including voices and avatars. Using a Research through Design approach, we conducted a week-long field study followed by interviews and design activities (N = 22), which uncovered how participants created diverse chatbot personas for emotional reliance, confronting stressors, connecting to intellectual discourse, reflecting mirrored selves, etc. We found that participants actively enriched the personas they constructed, shaping the dynamics between themselves and the chatbot to foster open and honest conversations. They also suggested other customizable features, such as integrating online activities and adjustable memory settings. Based on these findings, we discuss opportunities for enhancing personalized emotional support through emerging AI technologies.",International Conference on Human Factors in Computing Systems,10.1145/3706598.3713453,24,No,Directly relevant,chatbot; emotional support; well-being,Yes,1.0
