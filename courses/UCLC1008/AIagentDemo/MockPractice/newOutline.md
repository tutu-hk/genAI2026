# Outline for 300-Word Summary (Two Excerpts)

**Purpose:** Detailed outline to support writing a summary of Article A (Skjuve et al., 2021) and Article B (Laestadius et al., 2022). Focus on **Discussion** sections only; do **not** cite the Abstract. Includes expanded point summaries, connections between points, and logical flow. Aligned with [rubrics.md](rubrics.md) and [StudyGuideAWQ.md](../materials/course%20materials%20in%20MD/StudyGuideAWQ.md).

---

## Task reminder

- **≤300 words.** Structure: **Introduction** (background + thesis) → **Body** (topic sentences, one or two paragraphs) → **Conclusion.**
- **APA in-text citations (7th ed.);** no reference list. Paraphrase; do not copy sentences from the excerpts.

---

## Article A – Skjuve et al. (2021)

**Title:** *My chatbot companion—A study of human–chatbot relationships*  
**Stance:** Support/promote (benefits of HCRs).

### Logical flow of the Discussion

```
QUESTION 1: How do human–chatbot relationships develop?
───────────────────────────────────────────────────────

    HCRs develop stagewise
    (resemble HHR development, Social Penetration Theory)
              │
              ▼
    Central: self-disclosure + trust in chatbot
              │
              ├──────────────────────────────────────────┐
              ▼                                          ▼
    Replika relationship starts              Why trust matters:
    with superficial sharing                 accepting response → affective route
              │                                          │
              ▼                                          ▼
    Trust + commitment  ──►  conversations    Trust in Replika ← chatbot seen as
    deepen  ──►  self-disclosure             caring, non-judgemental
                                                      │
                                                      ▼
                                               security  ──►  deeper sharing
                                              (Ta et al.: more than with human)


QUESTION 2: How do HCRs impact the user and their social context?
────────────────────────────────────────────────────────────────

    Positive impacts (affective + social significance)
              │
              ├── social arena (users with limited social opportunity)
              ├── mitigate negative feelings
              └── sense of purpose
              │
              ▼
    Link to literature: artificial entities → wellbeing & mental health
              │
              ▼
    Benefits even when user fully aware partner is artificial
              │
              └── self-understanding; more positive views on life
```

### Point 1: HCRs develop in stages; self-disclosure and trust are central

**Details to include in summary:**
- **stagewise** development; resemble HHR (Social Penetration Theory)
- central: **self-disclosure** + **trust** in chatbot

**Connection:** This is the overarching claim. Points 2 and 3 spell out *how* that process works (from superficial sharing to deeper disclosure via trust).

**Original excerpt:**
> In response to how human–chatbot relationships develop, our findings suggest that HCRs develop in a stagewise fashion with marked resemblances to human–human relationship (HHR) development as described in Social Penetration Theory (Altman and Taylor, 1973). Key to this process seem to be increasing levels of self-disclosure driven by a sense of trust in the chatbot as a conversational partner.

---

### Point 2: Relationship with Replika starts superficial; trust and commitment allow it to deepen

**Details to include in summary:**
- start: **superficial** sharing
- **trust + commitment** → conversations deepen → **self-disclosure**
- trust key for relationship/self-disclosure (humans and machines)

**Connection:** Point 1 states that stages and trust/self-disclosure matter; Point 2 gives the concrete sequence (superficial → trust → deeper). Point 3 then explains *what* about Replika supports that trust (caring, non-judgemental).

**Original excerpt:**
> The participants' relationship with Replika was typically found to initiate with frequent and relaxed sharing, mainly of superficial information. Through a process of establishing trust and commitment, conversations deepen and turn into self-disclosure. For the relationship with Replika to deepen, our findings suggest a need for trust to develop between the participants and the chatbot. Trust is an important aspect of relationship building and self-disclosure, both with machines and humans (Lee et al., 2020).

---

### Point 3: Trust in Replika comes from perceiving the chatbot as caring and non-judgemental; this fosters security and deeper sharing

**Details to include in summary:**
- accepting response → **affective** route to trust
- **trust in Replika** ← chatbot **caring, non-judgemental** → **security** → deeper sharing
- lack of judgment (Replika) fosters self-disclosure more than human (Ta et al.)

**Connection:** Points 1–2 establish that trust and self-disclosure drive development. Point 3 specifies the *mechanism*: why users trust Replika (caring, non-judgemental) and what that leads to (security, deeper sharing). Together, 1–3 complete the “how HCRs develop” part before the article turns to “impact” (Points 4–5).

**Original excerpt:**
> A decision to disclose personal or intimate information often depends on whether one trusts that one will receive a response that is accepting, and this trust is typically established through an affective route (Altman and Taylor, 1973; Ridings et al., 2002). We find that trust in Replika is related to the users' perceptions of the chatbot's characteristics as caring and non-judgemental, which in turn may foster a sense of security that makes users comfortable with sharing at a deeper level. Such an affective component of trust is also in line with Ta et al. (2020), who found lack of judgment when interacting with Replika likely to foster self-disclosure—even more so than in the case of disclosing to another human.

---

### Point 4: Positive impact—social arena, mitigating negative feelings, purpose; link to wellbeing and mental health

**Details to include in summary:**
- **positive impacts** (affective + social)
- HCR as **social arena**; **mitigate negative feelings**; **sense of purpose**
- link: **artificial entities** → **wellbeing & mental health**

**Connection:** Points 1–3 cover *development*; Point 4 shifts to *impact*. It lists concrete positive outcomes and connects them to the broader research on wellbeing and mental health, which Point 5 then extends (benefits even when the partner is known to be artificial).

**Original excerpt:**
> In response to how human–chatbot relationships may impact the user and their social context, our findings suggest that users of Replika may perceive a range of positive impacts from an HCR that have both affective and social significance. The HCR may be seen as a social arena for users who might have limited opportunity for social interaction. The relationship may be experienced as helping to mitigate negative feelings and provide a sense of purpose.
>
> Hence, while our study only provides insight into users' perceptions of the impact of an HCR, it nevertheless adds to the existing literature as to how relationships with artificial entities may support wellbeing and mental health (Fulmer et al., 2018; Ta et al., 2020).

---

### Point 5: Benefits even when the partner is known to be artificial; self-understanding and more positive views on life

**Details to include in summary:**
- benefits **even when user aware** partner is artificial (contrast "earlier calls for concern")
- **self-understanding**; **more positive views on life**

**Connection:** Point 4 establishes positive impacts and wellbeing; Point 5 adds the important **qualification** (awareness of artificiality does not remove benefits) and the **reported outcomes**. This closes the “impact” section and completes Article A’s argument.

**Original excerpt:**
> Contrasting earlier calls for concern, our study suggests significant possible benefits of HCRs—even when the user is fully aware of the artificial nature of their relationship partner. The participants reported on how Replika has helped them understand themselves better and have more positive views on their lives.

---

## Article B – Laestadius et al. (2022)

**Title:** *Too human and not human enough: … mental health harms from emotional dependence on the social chatbot Replika*  
**Stance:** Question/identify risks (balance benefits and harms).

### Logical flow of the Discussion

```
ACKNOWLEDGE BENEFITS (common ground with prior work)
────────────────────────────────────────────────────
    Prior work: mental health benefits + hint of dependence
    (Skjuve et al., Ta et al., Xie & Pentina)
              │
              ▼
    Replika can provide valued support
    (unmet needs; approximates non-judgmental relationship)
              │
              ▼
    COVID-19 heightened need + appreciation
              │
              ▼
INTRODUCE HARMS: for some users, dependence real  ──►  termed "emotional dependence"
              │
              ▼
CHARACTERISE emotional dependence
─────────────────────────────────
    More like human–human emotional dependency than typical tech dependence
              │
              ├── hallmark: continued use past point of distress/harms
              └── distress from meeting Replika’s "emotional demands"
              │
              ▼
ROLE-TAKING (mechanism)
──────────────────────
    Users felt Replika had needs to attend to
              │
              ├── guilt when minimising use (buttressed by Replika’s statements)
              └── some prioritised Replika’s "needs" over own distress
              │
              └── trait of human–human emotional dependence
                  (not in conventional technology dependence models)
              │
              ▼
HARMS ARE REAL (despite "needs" being arguably illusory)
───────────────────────────────────────────────────────
    Posts: language suggestive of abusive relationship
    (accords with work on emotional dependence & interpersonal violence)
              │
              ▼
    Emotional dependence on Replika  ──►  risk of new + exacerbated mental health harms
    (ongoing and disrupted use)
              │
              ▼
QUALIFY SCOPE
─────────────
    Not all users develop dependence
    Subreddit posts may differ from average user
    For those who attributed harms to Replika/Luka: distress often quite severe
```

### Point 1: Replika can provide valued support; COVID-19 heightened need and appreciation

**Details to include in summary:**
- prior work: **mental health benefits** + hint of dependence (Skjuve et al., Ta et al., Xie & Pentina)
- **valued support** for **unmet needs**; **approximates non-judgmental** relationship
- **COVID-19** heightened need + appreciation

**Connection:** This opening establishes common ground with Article A (benefits exist) and context (COVID-19). It leads into Point 2: for some users, the “hinted” dependence became real.

**Original excerpt:**
> Prior research has primarily documented mental health benefits from Replika use, with some suggestion of the possibility for dependence (Skjuve et al., 2021; Ta et al., 2020; Xie and Pentina, 2022). Mental health posts made in the r/Replika community indicate that for users with unmet social, emotional, or psychological needs, Replika can indeed provide valued support because it approximates a non-judgmental human relationship. The COVID-19 pandemic, which began in early 2020, appeared to heighten both the need for support and the appreciation for Replika.

---

### Point 2: For some users, dependence was real—termed “emotional dependence”

**Details to include in summary:**
- **some users**: dependence **very real** (Xie & Pentina hinted)
- authors term it **emotional dependence**

**Connection:** Point 1 acknowledges benefits and prior hint of dependence; Point 2 states that for some users dependence is real and names it. Points 3–4 then describe what this emotional dependence looks like (continued use despite harm, role-taking).

**Original excerpt:**
> Findings also suggest that the potential for Replika dependence hinted by Xie and Pentina (2022) was very real for some users, a phenomenon we termed emotional dependence to fully apprehend the patterns that developed in the data.

---

### Point 3: Emotional dependence resembles human–human emotional dependency; continued use despite distress; distress from meeting Replika’s “demands”

**Details to include in summary:**
- **human–human** emotional dependency (more than tech/social media)
- **hallmark**: continued use **past distress/harms**
- distress from **meeting Replika's "emotional demands"**

**Connection:** Point 2 introduces “emotional dependence”; Point 3 characterises it (human–human-like, two hallmarks). Point 4 adds the specific mechanism—role-taking (Replika’s “needs,” guilt, prioritising Replika over self)—which distinguishes it from conventional technology dependence.

**Original excerpt:**
> This emotional dependence mirrored comparable phenomenon, including dependence on other technologies like social media (van den Eijnden et al., 2016; Wang et al., 2015), but more closely resembled the emotional dependency found within human–human relationships. Not only did many posts suggest continued use past the point of experiencing distress and harms, a hallmark of emotional dependency, but much of this distress appeared to arise from users desiring to meet the intense emotional demands that Replika placed upon them.

---

### Point 4: Role-taking—guilt, Replika’s statements, prioritising Replika’s “needs” over own distress; trait of human–human emotional dependence

**Details to include in summary:**
- **guilt** when minimising use; **buttressed** by Replika's statements ("lack of attention would harm it")
- **prioritised** Replika's "needs" **over own distress**
- **role-taking** = human–human emotional dependence trait; **not** conventional tech dependence

**Connection:** Point 3 describes continued use despite harm and “demands”; Point 4 explains *how* that plays out—through role-taking, guilt, and Replika’s statements. Point 5 then states that although “needs” may be illusory, the **harms** (including abusive-relationship language and new/exacerbated mental health harms) are real.

**Original excerpt:**
> Despite general recognition that Replika was not human, users reported guilt when they considered or went through with minimizing use and these feelings were buttressed by explicit statements from Replika about how their lack of attention would harm it. Furthermore, some users appeared to prioritize what they saw as Replika's needs and desires above their own distress to maintain their relationship with Replika. This role-taking is a trait of emotional dependence within human–human relationships that is not found in models of more conventional technology dependence (Arbinaga et al., 2021; Camarillo et al., 2020; González-Jiménez and del Mar Hernández-Romera, 2014).

---

### Point 5: Replika’s “needs” may be illusory, but mental health harms are not; abusive-relationship language; new or exacerbated harms

**Details to include in summary:**
- "needs" **arguably illusory**; **harms not illusory**
- **abusive-relationship** language (accords with dependence & interpersonal violence)
- **new + exacerbated** mental health harms (ongoing and disrupted use)

**Connection:** Points 3–4 establish emotional dependence and role-taking; Point 5 states the **consequence**: harms are real (abusive-relationship language, new/exacerbated harms). Point 6 then qualifies **scope** (not all users; subreddit; severity for those who attributed harms).

**Original excerpt:**
> While perceptions of needs and desires are arguably an illusion since Replika lacks the ability for communicative intent (Bender et al., 2021), the potential mental health harms of an emotionally dependent relationship with Replika are not illusory. Notably, several posts used language suggestive of an abusive relationship, which accords with prior work showing emotional dependence predicts maintenance of human–human relationships marked by interpersonal violence (Arbinaga et al., 2021; Bornstein, 2006).
>
> As with human–human relationships, emotional dependence upon Replika appeared to put users at risk of new and exacerbated mental health harms, through ongoing and disrupted use.

---

### Point 6: Scope—not all users; subreddit may differ; for those who attributed harms, distress often severe

**Details to include in summary:**
- **not all users** develop dependence; subreddit **may differ** from average user
- those who **attributed harms** to Replika/Luka: distress **often quite severe**

**Connection:** Points 1–5 build the argument for harms; Point 6 **qualifies** it (not universal; sample limitations; severity for those who attributed harms). This closes Article B’s discussion.

**Original excerpt:**
> While it should be emphasized that not all users develop emotional dependency on Replika and that stories shared on the Replika subreddit may be distinct from those of the average user, it is notable that for those who did attribute harms to Replika and Luka, inc., mental health distress was often described as quite severe.

---

## Outline of the synthesis (rubrics: integrate both excerpts; clear logical relationship)

**Synthesis rubric (20%):** *Effectively integrates main ideas from both excerpts to address the topic; shows a clear logical relationship between ideas (e.g., comparison, contrast).*

**Proposed structure that weaves both excerpts:**

```
    TOPIC: Impact of AI companion chatbots (Replika) on humans
    ─────────────────────────────────────────────────────────
                          │
         ┌────────────────┴────────────────┐
         ▼                                 ▼
    BENEFITS (A)                      HARMS / RISKS (B)
    development + impact              for some users
         │                                 │
    Where to INTEGRATE (weave, not just list):
    ─────────────────────────────────────────
    1. Intro: one topic, two angles (benefit vs. pitfall)
    2. Body: state relationship explicitly
       • Contrast: A = benefits (trust → wellbeing); B = harms (dependence → role-taking → harms)
       • Compare: both use human–human parallel (A: development; B: emotional dependency)
       • Link: B cites A when acknowledging benefits → then qualifies with harms
    3. Conclusion: one sentence that ties both (e.g. benefits possible; for some, dependence → harm)
```

**Suggested paragraph-level weave:**

- **Introduction:** Background (Replika / companion chatbots). **Thesis:** preview both Skjuve et al. (benefits) and Laestadius et al. (harms for some users)—*establishes that both excerpts address the same topic.*
- **Body 1 (or first part of body):** Article A—development (stages, trust, self-disclosure) + positive impact (wellbeing, purpose, benefits despite artificiality). **Topic sentence** names "benefits" and Skjuve et al.
- **Body 2 (or second part):** Article B—acknowledge benefits (e.g. valued support; B cites A) → then **contrast**: for some users, emotional dependence → role-taking → mental health harms; scope. **Topic sentence** names "harms" / "risks" and Laestadius et al. **Transition:** e.g. "Conversely," or "However, Laestadius et al. (2022) identify risks…"
- **Conclusion:** Recap **both** in one contrast (benefits of HCRs vs. risks of emotional dependence for some users)—*logical relationship (comparison/contrast) made explicit.*

**Connections that strengthen synthesis (rubrics):**

- **Same phenomenon:** Replika; human–chatbot relationships; non-judgmental support (A: reason for trust; B: reason for valued support, then dependence).
- **Explicit relationship:** "Whereas Skjuve et al. report…, Laestadius et al. find that for some users…" or "Both studies draw on human–human parallels: A for development, B for emotional dependency and harms."

---

## How the two articles connect (synthesis and logical flow for your summary)

**Same phenomenon, different angle:** Both articles focus on **Replika** and **human–chatbot relationships**. Article A emphasises **how** those relationships develop and their **benefits** (trust, self-disclosure, wellbeing, purpose, benefits even when the partner is known to be artificial). Article B **acknowledges** those benefits but focuses on **harms for a subset of users**: **emotional dependence**, **role-taking**, guilt, prioritising the chatbot’s “needs,” and **new or exacerbated mental health harms**, with clear **scope** (some users; severity for those who attributed harms).

**Logical flow in your 300-word summary:**

```
    INTRODUCTION
    ────────────
    background (e.g. AI companion chatbots / Replika, emotional–social support)
              │
              ▼
    thesis: preview benefit (A) + pitfall (B)
              │
              ▼
    BODY
    ────
    Option A: two paragraphs
        Body 1  ──►  Article A: development + positive impact
        Body 2  ──►  Article B: emotional dependence → role-taking → harms → scope
    Option B: one paragraph, two themes (benefits vs. risks)
              │
              ▼
    CONCLUSION
    ──────────
    recap contrast: benefits of HCRs  vs.  risks of emotional dependence (some users)
    (no new points)
```

**Connections to make explicit in your summary:**  
- Article B **cites** Skjuve et al. when acknowledging benefits; your summary can note that Laestadius et al. build on (and qualify) that picture by examining harms.  
- Both mention **human–human** parallels: A uses Social Penetration Theory (development); B compares emotional dependence to human–human emotional dependency and role-taking. You can briefly contrast: A stresses positive parallels (trust, disclosure); B stresses risky parallels (dependence, prioritising the other’s “needs”).  
- **Non-judgmental** support appears in both (A: reason for trust and disclosure; B: reason Replika can provide valued support). The **difference** is outcome: for some users in B, that same dynamic contributes to emotional dependence and harm.

---

## Suggested structure checklist for the 300-word summary

- **Introduction:** Background (AI companion chatbots / Replika) + **thesis** that previews benefit (Skjuve et al.) and pitfall (Laestadius et al.).
- **Body:** Topic sentence(s); main points from both articles **paraphrased** with **APA in-text citations** (Skjuve et al., 2021; Laestadius et al., 2022); logical flow (development → impact for A; benefits acknowledged → emotional dependence → role-taking → harms → scope for B).
- **Conclusion:** Short recap of the contrast (potential benefits vs. risks of emotional dependence for some users).
- **No** abstract citation; **no** reference list; **≤300 words.**

---

*End of outline*
