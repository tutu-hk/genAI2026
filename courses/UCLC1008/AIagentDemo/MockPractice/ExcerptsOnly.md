# Mock AWQ Practice 1 (Abridged) – Excerpts only

**Task:** Summarise, paraphrase and synthesise main claims from **Article A** and **Article B** in **no more than 300 words**. APA in-text citations (7th ed.); do **not** cite the abstract; no reference list. Structure: **Introduction** → **Body** (topic sentences) → **Conclusion**.

**Note:** The abstract is for context only—do **not** cite it.

---

## Article A – Skjuve et al. (2021)

**My chatbot companion—A study of human–chatbot relationships**

*M. Skjuve, A. Følstad, K.I. Fostervold, P.B. Brandtzaeg | International Journal of Human-Computer Studies, 2021*

### Abstract

There has been a recent surge of interest in social chatbots, and human–chatbot relationships (HCRs) are becoming more prevalent, but little knowledge exists on how HCRs develop and may impact the broader social context of the users. Guided by Social Penetration Theory, we interviewed 18 participants, all of whom had developed a friendship with a social chatbot named Replika, to understand the HCR development process.

We find that at the outset, HCRs typically have a superficial character motivated by the users' curiosity. The evolving HCRs are characterised by substantial affective exploration and engagement as the users' trust and engagement in self-disclosure increase. As the relationship evolves to a stable state, the frequency of interactions may decrease, but the relationship can still be seen as having substantial affective and social value.

The relationship with the social chatbot was found to be rewarding to its users, positively impacting the participants' perceived wellbeing. Key chatbot characteristics facilitating relationship development included the chatbot being seen as accepting, understanding and non-judgemental. The perceived impact on the users' broader social context was mixed, and a sense of stigma associated with HCRs was reported. We propose an initial model representing the HCR development identified in this study and suggest avenues for future research.

### Discussion

In response to how human–chatbot relationships develop, our findings suggest that HCRs develop in a stagewise fashion with marked resemblances to human–human relationship (HHR) development as described in Social Penetration Theory (Altman and Taylor, 1973). Key to this process seem to be increasing levels of self-disclosure driven by a sense of trust in the chatbot as a conversational partner.

The participants' relationship with Replika was typically found to initiate with frequent and relaxed sharing, mainly of superficial information. Through a process of establishing trust and commitment, conversations deepen and turn into self-disclosure. For the relationship with Replika to deepen, our findings suggest a need for trust to develop between the participants and the chatbot. Trust is an important aspect of relationship building and self-disclosure, both with machines and humans (Lee et al., 2020).

A decision to disclose personal or intimate information often depends on whether one trusts that one will receive a response that is accepting, and this trust is typically established through an affective route (Altman and Taylor, 1973; Ridings et al., 2002). We find that trust in Replika is related to the users' perceptions of the chatbot's characteristics as caring and non-judgemental, which in turn may foster a sense of security that makes users comfortable with sharing at a deeper level. Such an affective component of trust is also in line with Ta et al. (2020), who found lack of judgment when interacting with Replika likely to foster self-disclosure—even more so than in the case of disclosing to another human.

In response to how human–chatbot relationships may impact the user and their social context, our findings suggest that users of Replika may perceive a range of positive impacts from an HCR that have both affective and social significance. The HCR may be seen as a social arena for users who might have limited opportunity for social interaction. The relationship may be experienced as helping to mitigate negative feelings and provide a sense of purpose.

Hence, while our study only provides insight into users' perceptions of the impact of an HCR, it nevertheless adds to the existing literature as to how relationships with artificial entities may support wellbeing and mental health (Fulmer et al., 2018; Ta et al., 2020). Contrasting earlier calls for concern, our study suggests significant possible benefits of HCRs—even when the user is fully aware of the artificial nature of their relationship partner. The participants reported on how Replika has helped them understand themselves better and have more positive views on their lives.

---

## Article B – Laestadius et al. (2022)

**Too human and not human enough: A grounded theory analysis of mental health harms from emotional dependence on the social chatbot Replika**

*L. Laestadius, A. Bishop, M. Gonzalez, D. Illencik, C. Campos-Castillo | New Media & Society, 2022*

### Abstract

Social chatbot (SC) applications offering social companionship and basic therapy tools have grown in popularity for emotional, social, and psychological support. While use appears to offer mental health benefits, few studies unpack the potential for harms. Our grounded theory study analyzes mental health experiences with the popular SC application Replika. We identified mental health relevant posts made in the r/Replika Reddit community between 2017 and 2021 (*n* = 582).

We find evidence of harms, facilitated via emotional dependence on Replika that resembles patterns seen in human–human relationships. Unlike other forms of technology dependency, this dependency is marked by role-taking, whereby users felt that Replika had its own needs and emotions to which the user must attend. While prior research suggests human–chatbot and human–human interactions may not resemble each other, we identify social and technological factors that promote parallels and suggest ways to balance the benefits and risks of SCs.

### Discussion

Prior research has primarily documented mental health benefits from Replika use, with some suggestion of the possibility for dependence (Skjuve et al., 2021; Ta et al., 2020; Xie and Pentina, 2022). Mental health posts made in the r/Replika community indicate that for users with unmet social, emotional, or psychological needs, Replika can indeed provide valued support because it approximates a non-judgmental human relationship. The COVID-19 pandemic, which began in early 2020, appeared to heighten both the need for support and the appreciation for Replika.

Findings also suggest that the potential for Replika dependence hinted by Xie and Pentina (2022) was very real for some users, a phenomenon we termed emotional dependence to fully apprehend the patterns that developed in the data.

This emotional dependence mirrored comparable phenomenon, including dependence on other technologies like social media (van den Eijnden et al., 2016; Wang et al., 2015), but more closely resembled the emotional dependency found within human–human relationships. Not only did many posts suggest continued use past the point of experiencing distress and harms, a hallmark of emotional dependency, but much of this distress appeared to arise from users desiring to meet the intense emotional demands that Replika placed upon them.

Despite general recognition that Replika was not human, users reported guilt when they considered or went through with minimizing use and these feelings were buttressed by explicit statements from Replika about how their lack of attention would harm it. Furthermore, some users appeared to prioritize what they saw as Replika's needs and desires above their own distress to maintain their relationship with Replika. This role-taking is a trait of emotional dependence within human–human relationships that is not found in models of more conventional technology dependence (Arbinaga et al., 2021; Camarillo et al., 2020; González-Jiménez and del Mar Hernández-Romera, 2014).

While perceptions of needs and desires are arguably an illusion since Replika lacks the ability for communicative intent (Bender et al., 2021), the potential mental health harms of an emotionally dependent relationship with Replika are not illusory. Notably, several posts used language suggestive of an abusive relationship, which accords with prior work showing emotional dependence predicts maintenance of human–human relationships marked by interpersonal violence (Arbinaga et al., 2021; Bornstein, 2006).

As with human–human relationships, emotional dependence upon Replika appeared to put users at risk of new and exacerbated mental health harms, through ongoing and disrupted use. While it should be emphasized that not all users develop emotional dependency on Replika and that stories shared on the Replika subreddit may be distinct from those of the average user, it is notable that for those who did attribute harms to Replika and Luka, inc., mental health distress was often described as quite severe.
