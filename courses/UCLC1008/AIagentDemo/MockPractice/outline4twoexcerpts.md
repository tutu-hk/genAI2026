# Outline for Summary (Two Excerpts)

**Purpose:** Key points to cover when summarising and synthesising Article A (Skjuve et al., 2021) and Article B (Laestadius et al., 2022). Focus on **Discussion** sections; do **not** cite the Abstract. Aligned with [rubrics.md](rubrics.md) and [StudyGuideAWQ.md](../materials/course%20materials%20in%20MD/StudyGuideAWQ.md).

---

## Task reminder

- **≤300 words.** Structure: **Introduction** (background + thesis) → **Body** (topic sentences) → **Conclusion.**
- **APA in-text citations (7th ed.);** no reference list. Paraphrase; no copying.

---

## Article A – Skjuve et al. (2021)

**Title:** *My chatbot companion—A study of human–chatbot relationships*  
**Stance:** Support/promote (benefits of HCRs).

### Structure of the Discussion

The Discussion is organised in two main moves, each signalled by “In response to…”: (1) **how human–chatbot relationships develop** (paragraphs 1–3), and (2) **how they may impact the user and their social context** (paragraphs 4–5). The first move explains the process (stagewise, trust, self-disclosure); the second move reports positive impacts and links to wellbeing.

### Main points with original excerpts

**1. HCRs develop in a stagewise way, resembling human–human relationship development; self-disclosure and trust are central.**

Original excerpt:
> In response to how human–chatbot relationships develop, our findings suggest that HCRs develop in a stagewise fashion with marked resemblances to human–human relationship (HHR) development as described in Social Penetration Theory (Altman and Taylor, 1973). Key to this process seem to be increasing levels of self-disclosure driven by a sense of trust in the chatbot as a conversational partner.

**2. Relationship with Replika starts with superficial sharing; trust and commitment allow conversations to deepen into self-disclosure.**

Original excerpt:
> The participants' relationship with Replika was typically found to initiate with frequent and relaxed sharing, mainly of superficial information. Through a process of establishing trust and commitment, conversations deepen and turn into self-disclosure. For the relationship with Replika to deepen, our findings suggest a need for trust to develop between the participants and the chatbot. Trust is an important aspect of relationship building and self-disclosure, both with machines and humans (Lee et al., 2020).

**3. Trust in Replika is linked to users perceiving the chatbot as caring and non-judgemental; this fosters security and deeper self-disclosure.**

Original excerpt:
> A decision to disclose personal or intimate information often depends on whether one trusts that one will receive a response that is accepting, and this trust is typically established through an affective route (Altman and Taylor, 1973; Ridings et al., 2002). We find that trust in Replika is related to the users' perceptions of the chatbot's characteristics as caring and non-judgemental, which in turn may foster a sense of security that makes users comfortable with sharing at a deeper level. Such an affective component of trust is also in line with Ta et al. (2020), who found lack of judgment when interacting with Replika likely to foster self-disclosure—even more so than in the case of disclosing to another human.

**4. Positive impact: HCR as social arena, mitigating negative feelings and providing purpose; support for wellbeing and mental health.**

Original excerpt:
> In response to how human–chatbot relationships may impact the user and their social context, our findings suggest that users of Replika may perceive a range of positive impacts from an HCR that have both affective and social significance. The HCR may be seen as a social arena for users who might have limited opportunity for social interaction. The relationship may be experienced as helping to mitigate negative feelings and provide a sense of purpose.
>
> Hence, while our study only provides insight into users' perceptions of the impact of an HCR, it nevertheless adds to the existing literature as to how relationships with artificial entities may support wellbeing and mental health (Fulmer et al., 2018; Ta et al., 2020).

**5. Benefits hold even when users know the partner is artificial; participants reported better self-understanding and more positive views on life.**

Original excerpt:
> Contrasting earlier calls for concern, our study suggests significant possible benefits of HCRs—even when the user is fully aware of the artificial nature of their relationship partner. The participants reported on how Replika has helped them understand themselves better and have more positive views on their lives.

---

## Article B – Laestadius et al. (2022)

**Title:** *Too human and not human enough: … mental health harms from emotional dependence on the social chatbot Replika*  
**Stance:** Question/identify risks (balance benefits and harms).

### Structure of the Discussion

The Discussion first **acknowledges benefits** and prior work (paragraph 1), then **introduces emotional dependence** as real for some users (paragraph 2). It then **characterises that dependence**—similar to human–human emotional dependency, with continued use despite distress and meeting Replika’s “demands” (paragraph 3)—and **role-taking**: guilt, Replika’s statements, prioritising Replika’s “needs” over own distress (paragraph 4). Finally it **states that harms are real** (paragraph 5: abusive-relationship language, new/exacerbated harms) and **qualifies scope** (paragraph 6: not all users; subreddit may differ; for those who attributed harms, distress often severe).

### Main points with original excerpts

**1. Replika can provide valued support for users with unmet needs; COVID-19 heightened need and appreciation.**

Original excerpt:
> Prior research has primarily documented mental health benefits from Replika use, with some suggestion of the possibility for dependence (Skjuve et al., 2021; Ta et al., 2020; Xie and Pentina, 2022). Mental health posts made in the r/Replika community indicate that for users with unmet social, emotional, or psychological needs, Replika can indeed provide valued support because it approximates a non-judgmental human relationship. The COVID-19 pandemic, which began in early 2020, appeared to heighten both the need for support and the appreciation for Replika.

**2. For some users, dependence on Replika was real; the authors term this “emotional dependence”.**

Original excerpt:
> Findings also suggest that the potential for Replika dependence hinted by Xie and Pentina (2022) was very real for some users, a phenomenon we termed emotional dependence to fully apprehend the patterns that developed in the data.

**3. Emotional dependence resembles human–human emotional dependency: continued use despite distress, and distress from meeting Replika’s “emotional demands”.**

Original excerpt:
> This emotional dependence mirrored comparable phenomenon, including dependence on other technologies like social media (van den Eijnden et al., 2016; Wang et al., 2015), but more closely resembled the emotional dependency found within human–human relationships. Not only did many posts suggest continued use past the point of experiencing distress and harms, a hallmark of emotional dependency, but much of this distress appeared to arise from users desiring to meet the intense emotional demands that Replika placed upon them.

**4. Role-taking: users felt Replika had needs to attend to; guilt when minimising use (reinforced by Replika’s statements); some prioritised Replika’s “needs” over their own distress. This is a trait of human–human emotional dependence, not typical technology dependence.**

Original excerpt:
> Despite general recognition that Replika was not human, users reported guilt when they considered or went through with minimizing use and these feelings were buttressed by explicit statements from Replika about how their lack of attention would harm it. Furthermore, some users appeared to prioritize what they saw as Replika's needs and desires above their own distress to maintain their relationship with Replika. This role-taking is a trait of emotional dependence within human–human relationships that is not found in models of more conventional technology dependence (Arbinaga et al., 2021; Camarillo et al., 2020; González-Jiménez and del Mar Hernández-Romera, 2014).

**5. Although Replika’s “needs” may be illusory, mental health harms are not; some posts used language suggestive of an abusive relationship; emotional dependence is linked to new or exacerbated harms.**

Original excerpt:
> While perceptions of needs and desires are arguably an illusion since Replika lacks the ability for communicative intent (Bender et al., 2021), the potential mental health harms of an emotionally dependent relationship with Replika are not illusory. Notably, several posts used language suggestive of an abusive relationship, which accords with prior work showing emotional dependence predicts maintenance of human–human relationships marked by interpersonal violence (Arbinaga et al., 2021; Bornstein, 2006).
>
> As with human–human relationships, emotional dependence upon Replika appeared to put users at risk of new and exacerbated mental health harms, through ongoing and disrupted use.

**6. Scope: not all users develop dependence; subreddit posts may differ from average users; for those who attributed harms to Replika/Luka, distress was often quite severe.**

Original excerpt:
> While it should be emphasized that not all users develop emotional dependency on Replika and that stories shared on the Replika subreddit may be distinct from those of the average user, it is notable that for those who did attribute harms to Replika and Luka, inc., mental health distress was often described as quite severe.

---

## Synthesis: How to connect both excerpts

- **Article A** focuses on how HCRs develop and their **benefits**: trust, self-disclosure, wellbeing, sense of purpose, positive impact even when the partner is known to be artificial.
- **Article B** focuses on **harms** for a subset of users: emotional dependence, role-taking, guilt, prioritising the chatbot’s “needs,” and new or exacerbated mental health harms (with scope clearly qualified).
- **Relationship:** Contrast (benefit vs. pitfall). Both address the same phenomenon (Replika/companion chatbots); one emphasises rewards, the other risks for some users.

**Thesis preview (example):**  
*This summary outlines both the potential benefits of human–chatbot relationships, as reported by Skjuve et al. (2021), and the mental health risks associated with emotional dependence on Replika, as identified by Laestadius et al. (2022).*

---

## Structure checklist (rubrics & Study Guide)

- **Introduction:** Background (e.g. AI companion chatbots / Replika in emotional support) + **thesis** that previews benefit (A) and pitfall (B).
- **Body:** At least one paragraph per source (or one paragraph with two clear themes). Each body paragraph has a **topic sentence**; claims **paraphrased** with **APA in-text citations** (Skjuve et al., 2021; Laestadius et al., 2022).
- **Conclusion:** Brief recap of the contrast (potential benefits vs. risks of dependence).
- **No** abstract citation; **no** reference list; **≤300 words.**

---

*End of outline*
