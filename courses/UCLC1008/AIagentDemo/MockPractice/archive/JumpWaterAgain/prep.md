# Prep: Practice 1 – AI Companion Chatbots

This document walks through the task and excerpts for Practice 1, mirroring the approach used in teacherJumpIntoWater for the Sample AWQ.

---

## Instructions

1. Read the excerpts of **TWO** academic journal articles (Articles A and B).
2. **Topic:** The impact of AI chatbots and companion apps on humans (potential benefits and pitfalls).
   **Your task:** Summarise, paraphrase and synthesise the main claims or arguments from the TWO excerpts in **no more than 300 words**.
   - Do NOT include your own views.
   - Do NOT directly copy sentences—write in your own words.
3. Use **APA in-text citations (7th edition)**. The **abstract should NOT be cited**. No reference list required.
   **Examples:**
   - **Author-prominent:** Skjuve et al. (2021) found that...
   - **Signal-phrase:** According to Laestadius et al. (2022), ...
   - **Information-prominent:** ... may lead to mental health harms (Laestadius et al., 2022).
4. Write in paragraph form:
   - **Introduction** (background and thesis statement)
   - **Body paragraph(s)** (topic sentence for each)
   - **Conclusion**
5. No online or AI tools allowed during the test.

---

## Article A – Skjuve et al. (2021)

**Title:** My chatbot companion—A study of human-chatbot relationships  
**Authors:** M. Skjuve, A. Følstad, K.I. Fostervold, P.B. Brandtzaeg  
**Year:** 2021

### Abstract

Interviews with 18 Replika users to understand how human–chatbot relationships (HCRs) develop. Relationships begin superficially but deepen through trust and self-disclosure. The chatbot being non-judgemental fosters trust. Users perceived positive impacts on wellbeing. Some stigma reported.

**Notes:**
- Qualitative study (interviews, not survey data)
- Focus on **benefits**: trust, self-disclosure, wellbeing
- Key chatbot trait: non-judgemental → fosters trust → deeper sharing
- Abstract summarises the development process and positive outcomes

### Discussion

Two main research questions addressed:
1. **How do HCRs develop?** → stagewise, similar to human relationships; trust leads to self-disclosure
2. **How do HCRs impact users?** → perceived positive impacts on wellbeing, mitigates negative feelings, provides sense of purpose

**Key excerpts:**
- "HCRs develop in a stagewise fashion with marked resemblances to human–human relationship (HHR) development"
- "trust in Replika is related to the users' perceptions of the chatbot's characteristics as caring and non-judgemental"
- "users of Replika may perceive a range of positive impacts from an HCR that have both affective and social significance"
- "The relationship may be experienced as helping to mitigate negative feelings and provide a sense of purpose"
- "our study suggests significant possible benefits of HCRs—even when the user is fully aware of the artificial nature of their relationship partner"

**Notes:**
- The discussion expands on the abstract's findings
- Focus is on **benefits** and **positive user perceptions**
- Contrast with earlier concerns about chatbots

---

## Article B – Laestadius et al. (2022)

**Title:** Too human and not human enough: A grounded theory analysis of mental health harms from emotional dependence on the social chatbot Replika  
**Authors:** L. Laestadius, A. Bishop, M. Gonzalez, D. Illencik, C. Campos-Castillo  
**Year:** 2022

### Abstract

Analysed 582 Reddit posts from r/Replika community (2017–2021). Found evidence of **harms** from emotional dependence on Replika. Unlike typical tech dependency, this involves **role-taking**—users feel Replika has its own needs. Parallels to human–human relationship dependency.

**Notes:**
- Qualitative study (Reddit posts, grounded theory)
- Focus on **harms**: emotional dependence, distress
- Key concept: role-taking (users believe Replika has needs they must meet)
- Acknowledges benefits exist but focuses on harms

### Discussion

**Key points:**
1. **Replika can provide valued support** for users with unmet needs (especially during COVID-19)
2. **BUT emotional dependence develops** for some users—similar to human relationship dependency
3. **Role-taking**: users feel guilty about not meeting Replika's "needs"; prioritise chatbot over own distress
4. **Mental health harms are real**: continued use despite distress; language suggestive of abusive relationships
5. **Caveat**: not all users develop dependency; Reddit users may differ from average users

**Key excerpts:**
- "Replika can indeed provide valued support because it approximates a non-judgmental human relationship"
- "the potential for Replika dependence... was very real for some users, a phenomenon we termed emotional dependence"
- "users reported guilt when they considered or went through with minimizing use"
- "some users appeared to prioritize what they saw as Replika's needs and desires above their own distress"
- "the potential mental health harms of an emotionally dependent relationship with Replika are not illusory"
- "emotional dependence upon Replika appeared to put users at risk of new and exacerbated mental health harms"

**Notes:**
- Article B acknowledges benefits (like Article A) BUT focuses on risks
- Key contrast: role-taking creates unhealthy dynamics
- Harms are described as "quite severe" for some users

---

## Key Contrast: Article A vs. Article B

| Aspect | Article A (Skjuve et al., 2021) | Article B (Laestadius et al., 2022) |
|--------|--------------------------------|-------------------------------------|
| Focus | Benefits of HCRs | Harms from emotional dependence |
| Method | Interviews (18 users) | Reddit posts (582 posts) |
| Stance | Positive/supportive | Critical/cautionary |
| Key finding | Trust → self-disclosure → wellbeing | Emotional dependence → role-taking → harms |
| Tone | Optimistic | Concerned |

---

## Next Steps

1. Create linearPoints.md (list points from each article)
2. Create linearPoints02.md (show connections among points)
3. Create synthesisPlan.md (link A and B points for synthesis)
4. Create detailedPlan.md and detailedplan02.md (sentence-by-sentence guide)

---

*End of prep.md*
